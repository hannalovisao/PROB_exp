+ GPUS=1
+ RUN_COMMAND=configs/EVAL_M_OWOD_BENCHMARK.sh
+ '[' 1 -lt 8 ']'
+ GPUS_PER_NODE=1
+ MASTER_ADDR=127.0.0.1
+ MASTER_PORT=29501
+ NODE_RANK=0
+ let NNODES=GPUS/GPUS_PER_NODE
+ python ./tools/launch.py --nnodes 1 --node_rank 0 --master_addr 127.0.0.1 --master_port 29501 --nproc_per_node 1 configs/EVAL_M_OWOD_BENCHMARK.sh
running eval ofnano prob-detr, M-OWODB dataset
+ EXP_DIR=exps/MOWODB/PROB
+ PY_ARGS=
+ WANDB_NAME=PROB_V1
+ PY_ARGS=
+ python -u main_open_world.py --output_dir exps/MOWODB/PROB/eval --dataset TOWOD --PREV_INTRODUCED_CLS 0 --CUR_INTRODUCED_CLS 20 --train_set owod_t1_train --test_set owod_all_task_test --epochs 191 --lr_drop 35 --model_type prob --obj_loss_coef 8e-4 --obj_temp 1.3 --pretrain exps/MOWODB/PROB/t1.pth --eval --wandb_project ''
{'OWDETR': ('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown'), 'TOWOD': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown'), 'VOC2007': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown')}
('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown')
| distributed init (rank 0): env://
git:
  sha: 5675c9e44c999734c9465041c4c1f191dda931f9, status: has uncommited changes, branch: main

Namespace(lr=0.0002, lr_backbone_names=['backbone.0'], lr_backbone=2e-05, lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, batch_size=5, weight_decay=0.0001, epochs=191, lr_drop=35, lr_drop_epochs=None, clip_max_norm=0.1, sgd=False, with_box_refine=False, two_stage=False, masks=False, backbone='dino_resnet50', frozen_weights=None, dilation=False, position_embedding='sine', position_embedding_scale=6.283185307179586, num_feature_levels=4, enc_layers=6, dec_layers=6, dim_feedforward=1024, hidden_dim=256, dropout=0.1, nheads=8, num_queries=100, dec_n_points=4, enc_n_points=4, aux_loss=True, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, cls_loss_coef=2, bbox_loss_coef=5, giou_loss_coef=2, focal_alpha=0.25, coco_panoptic_path=None, remove_difficult=False, output_dir='exps/MOWODB/PROB/eval', device='cuda', seed=42, resume='', start_epoch=0, eval=True, viz=False, eval_every=5, num_workers=3, cache_mode=False, PREV_INTRODUCED_CLS=0, CUR_INTRODUCED_CLS=20, unmatched_boxes=False, top_unk=5, featdim=1024, invalid_cls_logits=False, NC_branch=False, bbox_thresh=0.3, pretrain='exps/MOWODB/PROB/t1.pth', nc_loss_coef=2, train_set='owod_t1_train', test_set='owod_all_task_test', num_classes=81, nc_epoch=0, dataset='TOWOD', data_root='./data/OWOD', unk_conf_w=1.0, model_type='prob', wandb_name='', wandb_project='', obj_loss_coef=0.0008, obj_temp=1.3, freeze_prob_model=False, num_inst_per_class=50, exemplar_replay_selection=False, exemplar_replay_max_length=10000000000.0, exemplar_replay_dir='', exemplar_replay_prev_file='', exemplar_replay_cur_file='', exemplar_replay_random=False, rank=0, world_size=1, gpu=0, dist_url='env://', distributed=True, dist_backend='nccl')
Invalid class range: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]
DINO resnet50
/home/hannalukas/miniconda3/envs/prob/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/hannalukas/miniconda3/envs/prob/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
running with exemplar_replay_selection
DeformableDETR(
  (transformer): DeformableTransformer(
    (encoder): DeformableTransformerEncoder(
      (layers): ModuleList(
        (0): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (decoder): DeformableTransformerDecoder(
      (layers): ModuleList(
        (0): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (reference_points): Linear(in_features=256, out_features=2, bias=True)
  )
  (class_embed): ModuleList(
    (0): Linear(in_features=256, out_features=81, bias=True)
    (1): Linear(in_features=256, out_features=81, bias=True)
    (2): Linear(in_features=256, out_features=81, bias=True)
    (3): Linear(in_features=256, out_features=81, bias=True)
    (4): Linear(in_features=256, out_features=81, bias=True)
    (5): Linear(in_features=256, out_features=81, bias=True)
  )
  (bbox_embed): ModuleList(
    (0): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (1): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (2): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (3): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (4): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (5): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (prob_obj_head): ModuleList(
    (0): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (1): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (2): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (3): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (4): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (5): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (query_embed): Embedding(100, 512)
  (input_proj): ModuleList(
    (0): Sequential(
      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (1): Sequential(
      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (2): Sequential(
      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (3): Sequential(
      (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
  )
  (backbone): Joiner(
    (0): Backbone(
      (body): IntermediateLayerGetter(
        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
        (bn1): FrozenBatchNorm2d()
        (relu): ReLU(inplace=True)
        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
      )
    )
    (1): PositionEmbeddingSine()
  )
)
number of params: 39742295
TOWOD
owod_t1_train
owod_all_task_test
Dataset OWDetection
    Number of datapoints: 16551
    Root location: ./data/OWOD
    [['train'], Compose(
    <datasets.transforms.RandomHorizontalFlip object at 0x7ab2fdf39900>
    <datasets.transforms.RandomSelect object at 0x7ab30fc1ae30>
    Compose(
    <datasets.transforms.ToTensor object at 0x7ab2fdf4eef0>
    <datasets.transforms.Normalize object at 0x7ab3ddd80b50>
)
)]
Dataset OWDetection
    Number of datapoints: 10246
    Root location: ./data/OWOD
    [['test'], Compose(
    <datasets.transforms.RandomResize object at 0x7ab30fc1a980>
    Compose(
    <datasets.transforms.ToTensor object at 0x7ab30fc1a950>
    <datasets.transforms.Normalize object at 0x7ab30fc1ab60>
)
)]
Initialized from the pre-training model
<All keys matched successfully>
testing data details
81
80
('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor')
('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown')
/home/hannalukas/Documents/PROB/PROB_exp/models/position_encoding.py:49: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)
/home/hannalukas/miniconda3/envs/prob/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/hannalukas/Documents/PROB/PROB_exp/models/prob_deformable_detr.py:537: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  topk_boxes = topk_indexes // out_logits.shape[2]
Test:  [   0/2050]  eta: 1:13:22    time: 2.1476  data: 0.4219  max mem: 2694
Test:  [  10/2050]  eta: 0:35:35    time: 1.0466  data: 0.0469  max mem: 3324
Test:  [  20/2050]  eta: 0:33:12    time: 0.9231  data: 0.0096  max mem: 3324
Test:  [  30/2050]  eta: 0:31:43    time: 0.8850  data: 0.0094  max mem: 3324
Test:  [  40/2050]  eta: 0:31:12    time: 0.8794  data: 0.0091  max mem: 3324
Test:  [  50/2050]  eta: 0:30:23    time: 0.8642  data: 0.0091  max mem: 3324
Test:  [  60/2050]  eta: 0:29:56    time: 0.8436  data: 0.0089  max mem: 3324
Test:  [  70/2050]  eta: 0:29:15    time: 0.8223  data: 0.0086  max mem: 3324
Test:  [  80/2050]  eta: 0:28:53    time: 0.8108  data: 0.0085  max mem: 3324
Test:  [  90/2050]  eta: 0:29:07    time: 0.9094  data: 0.0095  max mem: 3324
Test:  [ 100/2050]  eta: 0:28:44    time: 0.9017  data: 0.0094  max mem: 3324
Test:  [ 110/2050]  eta: 0:28:39    time: 0.8630  data: 0.0093  max mem: 3324
Test:  [ 120/2050]  eta: 0:28:30    time: 0.8954  data: 0.0096  max mem: 3324
Test:  [ 130/2050]  eta: 0:28:09    time: 0.8453  data: 0.0092  max mem: 3324
Test:  [ 140/2050]  eta: 0:27:47    time: 0.7929  data: 0.0089  max mem: 3324
Test:  [ 150/2050]  eta: 0:27:35    time: 0.8146  data: 0.0089  max mem: 3324
Test:  [ 160/2050]  eta: 0:27:28    time: 0.8668  data: 0.0093  max mem: 3324
Test:  [ 170/2050]  eta: 0:27:26    time: 0.9068  data: 0.0098  max mem: 3324
Test:  [ 180/2050]  eta: 0:27:08    time: 0.8627  data: 0.0094  max mem: 3324
Test:  [ 190/2050]  eta: 0:27:00    time: 0.8321  data: 0.0093  max mem: 3324
Test:  [ 200/2050]  eta: 0:26:52    time: 0.8785  data: 0.0098  max mem: 3324
Test:  [ 210/2050]  eta: 0:26:45    time: 0.8868  data: 0.0097  max mem: 3324
Test:  [ 220/2050]  eta: 0:26:33    time: 0.8570  data: 0.0097  max mem: 3324
Test:  [ 230/2050]  eta: 0:26:24    time: 0.8513  data: 0.0096  max mem: 3324
Test:  [ 240/2050]  eta: 0:26:17    time: 0.8848  data: 0.0096  max mem: 3324
Test:  [ 250/2050]  eta: 0:26:12    time: 0.9049  data: 0.0100  max mem: 3324
Test:  [ 260/2050]  eta: 0:26:04    time: 0.9001  data: 0.0101  max mem: 3324
Test:  [ 270/2050]  eta: 0:25:53    time: 0.8622  data: 0.0095  max mem: 3324
Test:  [ 280/2050]  eta: 0:25:42    time: 0.8410  data: 0.0095  max mem: 3324
Test:  [ 290/2050]  eta: 0:25:36    time: 0.8825  data: 0.0096  max mem: 3324
Test:  [ 300/2050]  eta: 0:25:32    time: 0.9364  data: 0.0102  max mem: 3324
Test:  [ 310/2050]  eta: 0:25:22    time: 0.9021  data: 0.0102  max mem: 3324
Test:  [ 320/2050]  eta: 0:25:14    time: 0.8716  data: 0.0097  max mem: 3324
Test:  [ 330/2050]  eta: 0:25:08    time: 0.9046  data: 0.0100  max mem: 3324
Test:  [ 340/2050]  eta: 0:25:00    time: 0.9093  data: 0.0100  max mem: 3324
Test:  [ 350/2050]  eta: 0:24:48    time: 0.8530  data: 0.0096  max mem: 3324
Test:  [ 360/2050]  eta: 0:24:40    time: 0.8481  data: 0.0095  max mem: 3324
Test:  [ 370/2050]  eta: 0:24:28    time: 0.8457  data: 0.0093  max mem: 3324
Test:  [ 380/2050]  eta: 0:24:16    time: 0.8017  data: 0.0090  max mem: 3324
Test:  [ 390/2050]  eta: 0:24:09    time: 0.8637  data: 0.0097  max mem: 3324
Test:  [ 400/2050]  eta: 0:24:01    time: 0.9049  data: 0.0101  max mem: 3324
Test:  [ 410/2050]  eta: 0:23:53    time: 0.8863  data: 0.0099  max mem: 3324
Test:  [ 420/2050]  eta: 0:23:46    time: 0.8991  data: 0.0098  max mem: 3324
Test:  [ 430/2050]  eta: 0:23:36    time: 0.8858  data: 0.0099  max mem: 3324
Test:  [ 440/2050]  eta: 0:23:28    time: 0.8800  data: 0.0104  max mem: 3324
Test:  [ 450/2050]  eta: 0:23:19    time: 0.8743  data: 0.0109  max mem: 3324
Test:  [ 460/2050]  eta: 0:23:09    time: 0.8537  data: 0.0110  max mem: 3324
Test:  [ 470/2050]  eta: 0:22:59    time: 0.8398  data: 0.0102  max mem: 3324
Test:  [ 480/2050]  eta: 0:22:53    time: 0.8826  data: 0.0099  max mem: 3324
Test:  [ 490/2050]  eta: 0:22:47    time: 0.9571  data: 0.0105  max mem: 3324
Test:  [ 500/2050]  eta: 0:22:39    time: 0.9337  data: 0.0105  max mem: 3324
Test:  [ 510/2050]  eta: 0:22:27    time: 0.8402  data: 0.0097  max mem: 3324
Test:  [ 520/2050]  eta: 0:22:16    time: 0.7932  data: 0.0094  max mem: 3324
Test:  [ 530/2050]  eta: 0:22:06    time: 0.8179  data: 0.0098  max mem: 3324
Test:  [ 540/2050]  eta: 0:22:00    time: 0.8903  data: 0.0105  max mem: 3324
Test:  [ 550/2050]  eta: 0:21:51    time: 0.9201  data: 0.0106  max mem: 3324
Test:  [ 560/2050]  eta: 0:21:44    time: 0.9140  data: 0.0107  max mem: 3627
Test:  [ 570/2050]  eta: 0:21:36    time: 0.9085  data: 0.0106  max mem: 3627
Test:  [ 580/2050]  eta: 0:21:26    time: 0.8553  data: 0.0099  max mem: 3627
Test:  [ 590/2050]  eta: 0:21:17    time: 0.8540  data: 0.0106  max mem: 3627
Test:  [ 600/2050]  eta: 0:21:08    time: 0.8662  data: 0.0111  max mem: 3627
Test:  [ 610/2050]  eta: 0:21:00    time: 0.8851  data: 0.0115  max mem: 3627
Test:  [ 620/2050]  eta: 0:20:52    time: 0.9137  data: 0.0122  max mem: 3627
Test:  [ 630/2050]  eta: 0:20:43    time: 0.8769  data: 0.0111  max mem: 3627
Test:  [ 640/2050]  eta: 0:20:34    time: 0.8613  data: 0.0105  max mem: 3627
Test:  [ 650/2050]  eta: 0:20:26    time: 0.9036  data: 0.0113  max mem: 3627
Test:  [ 660/2050]  eta: 0:20:19    time: 0.9367  data: 0.0110  max mem: 3627
Test:  [ 670/2050]  eta: 0:20:09    time: 0.8869  data: 0.0097  max mem: 3627
Test:  [ 680/2050]  eta: 0:20:01    time: 0.8604  data: 0.0099  max mem: 3627
Test:  [ 690/2050]  eta: 0:19:51    time: 0.8696  data: 0.0098  max mem: 3627
Test:  [ 700/2050]  eta: 0:19:43    time: 0.8740  data: 0.0097  max mem: 3627
Test:  [ 710/2050]  eta: 0:19:35    time: 0.9029  data: 0.0106  max mem: 3627
Test:  [ 720/2050]  eta: 0:19:26    time: 0.8950  data: 0.0106  max mem: 3627
Test:  [ 730/2050]  eta: 0:19:18    time: 0.8909  data: 0.0103  max mem: 3627
Test:  [ 740/2050]  eta: 0:19:09    time: 0.8824  data: 0.0103  max mem: 3627
Test:  [ 750/2050]  eta: 0:18:59    time: 0.8372  data: 0.0099  max mem: 3627
Test:  [ 760/2050]  eta: 0:18:52    time: 0.8997  data: 0.0103  max mem: 3627
Test:  [ 770/2050]  eta: 0:18:44    time: 0.9415  data: 0.0109  max mem: 3627
Test:  [ 780/2050]  eta: 0:18:34    time: 0.8693  data: 0.0102  max mem: 3627
Test:  [ 790/2050]  eta: 0:18:25    time: 0.8440  data: 0.0096  max mem: 3627
Test:  [ 800/2050]  eta: 0:18:17    time: 0.8902  data: 0.0103  max mem: 3627
Test:  [ 810/2050]  eta: 0:18:08    time: 0.9117  data: 0.0106  max mem: 3627
Test:  [ 820/2050]  eta: 0:17:59    time: 0.8530  data: 0.0100  max mem: 3627
Test:  [ 830/2050]  eta: 0:17:50    time: 0.8558  data: 0.0101  max mem: 3627
Test:  [ 840/2050]  eta: 0:17:40    time: 0.8485  data: 0.0098  max mem: 3627
Test:  [ 850/2050]  eta: 0:17:32    time: 0.8670  data: 0.0101  max mem: 3627
Test:  [ 860/2050]  eta: 0:17:25    time: 0.9427  data: 0.0112  max mem: 3627
Test:  [ 870/2050]  eta: 0:17:16    time: 0.9182  data: 0.0108  max mem: 3627
Test:  [ 880/2050]  eta: 0:17:06    time: 0.8239  data: 0.0094  max mem: 3627
Test:  [ 890/2050]  eta: 0:16:57    time: 0.8363  data: 0.0095  max mem: 3627
Test:  [ 900/2050]  eta: 0:16:48    time: 0.8882  data: 0.0103  max mem: 3627
Test:  [ 910/2050]  eta: 0:16:39    time: 0.8449  data: 0.0100  max mem: 3627
Test:  [ 920/2050]  eta: 0:16:30    time: 0.8212  data: 0.0096  max mem: 3627
Test:  [ 930/2050]  eta: 0:16:21    time: 0.8485  data: 0.0099  max mem: 3627
Test:  [ 940/2050]  eta: 0:16:12    time: 0.8937  data: 0.0104  max mem: 3627
Test:  [ 950/2050]  eta: 0:16:04    time: 0.8947  data: 0.0102  max mem: 3627
Test:  [ 960/2050]  eta: 0:15:54    time: 0.8561  data: 0.0100  max mem: 3627
Test:  [ 970/2050]  eta: 0:15:45    time: 0.8431  data: 0.0101  max mem: 3627
Test:  [ 980/2050]  eta: 0:15:36    time: 0.8301  data: 0.0099  max mem: 3627
Test:  [ 990/2050]  eta: 0:15:27    time: 0.8418  data: 0.0097  max mem: 3627
Test:  [1000/2050]  eta: 0:15:19    time: 0.9136  data: 0.0104  max mem: 3627
Test:  [1010/2050]  eta: 0:15:12    time: 0.9762  data: 0.0113  max mem: 3627
Test:  [1020/2050]  eta: 0:15:03    time: 0.9547  data: 0.0111  max mem: 3627
Test:  [1030/2050]  eta: 0:14:55    time: 0.9283  data: 0.0108  max mem: 3627
Test:  [1040/2050]  eta: 0:14:47    time: 0.9548  data: 0.0109  max mem: 3627
Test:  [1050/2050]  eta: 0:14:40    time: 0.9913  data: 0.0113  max mem: 3627
Test:  [1060/2050]  eta: 0:14:31    time: 0.9475  data: 0.0113  max mem: 3627
Test:  [1070/2050]  eta: 0:14:22    time: 0.8515  data: 0.0108  max mem: 3627
Test:  [1080/2050]  eta: 0:14:13    time: 0.8510  data: 0.0108  max mem: 3627
Test:  [1090/2050]  eta: 0:14:04    time: 0.8726  data: 0.0111  max mem: 3627
Test:  [1100/2050]  eta: 0:13:56    time: 0.9403  data: 0.0116  max mem: 3627
Test:  [1110/2050]  eta: 0:13:47    time: 0.9275  data: 0.0112  max mem: 3627
Test:  [1120/2050]  eta: 0:13:39    time: 0.9225  data: 0.0114  max mem: 3627
Test:  [1130/2050]  eta: 0:13:30    time: 0.9197  data: 0.0115  max mem: 3627
Test:  [1140/2050]  eta: 0:13:21    time: 0.8223  data: 0.0103  max mem: 3627
Test:  [1150/2050]  eta: 0:13:12    time: 0.8346  data: 0.0102  max mem: 3627
Test:  [1160/2050]  eta: 0:13:03    time: 0.9028  data: 0.0114  max mem: 3627
Test:  [1170/2050]  eta: 0:12:55    time: 0.9227  data: 0.0117  max mem: 3627
Test:  [1180/2050]  eta: 0:12:47    time: 0.9329  data: 0.0111  max mem: 3627
Test:  [1190/2050]  eta: 0:12:38    time: 0.9202  data: 0.0115  max mem: 3627
Test:  [1200/2050]  eta: 0:12:30    time: 0.9309  data: 0.0116  max mem: 3627
Test:  [1210/2050]  eta: 0:12:22    time: 0.9830  data: 0.0118  max mem: 3627
Test:  [1220/2050]  eta: 0:12:13    time: 0.9897  data: 0.0123  max mem: 3627
Test:  [1230/2050]  eta: 0:12:06    time: 1.0269  data: 0.0127  max mem: 3627
Test:  [1240/2050]  eta: 0:11:57    time: 1.0088  data: 0.0123  max mem: 3627
Test:  [1250/2050]  eta: 0:11:49    time: 0.9343  data: 0.0119  max mem: 3627
Test:  [1260/2050]  eta: 0:11:40    time: 0.9235  data: 0.0116  max mem: 3627
Test:  [1270/2050]  eta: 0:11:31    time: 0.8955  data: 0.0109  max mem: 3627
Test:  [1280/2050]  eta: 0:11:22    time: 0.8890  data: 0.0111  max mem: 3627
Test:  [1290/2050]  eta: 0:11:13    time: 0.8914  data: 0.0112  max mem: 3627
Test:  [1300/2050]  eta: 0:11:04    time: 0.8608  data: 0.0109  max mem: 3627
Test:  [1310/2050]  eta: 0:10:56    time: 0.8807  data: 0.0108  max mem: 3627
Test:  [1320/2050]  eta: 0:10:47    time: 0.8915  data: 0.0112  max mem: 3627
Test:  [1330/2050]  eta: 0:10:38    time: 0.9350  data: 0.0115  max mem: 3627
Test:  [1340/2050]  eta: 0:10:29    time: 0.9241  data: 0.0112  max mem: 3627
Test:  [1350/2050]  eta: 0:10:21    time: 0.9142  data: 0.0115  max mem: 3627
Test:  [1360/2050]  eta: 0:10:12    time: 0.9002  data: 0.0112  max mem: 3627
Test:  [1370/2050]  eta: 0:10:03    time: 0.8417  data: 0.0106  max mem: 3627
Test:  [1380/2050]  eta: 0:09:54    time: 0.8964  data: 0.0112  max mem: 3627
Test:  [1390/2050]  eta: 0:09:45    time: 0.9132  data: 0.0114  max mem: 3627
Test:  [1400/2050]  eta: 0:09:36    time: 0.8597  data: 0.0108  max mem: 3627
Test:  [1410/2050]  eta: 0:09:27    time: 0.8772  data: 0.0105  max mem: 3627
Test:  [1420/2050]  eta: 0:09:19    time: 0.9055  data: 0.0110  max mem: 3627
Test:  [1430/2050]  eta: 0:09:10    time: 0.9679  data: 0.0119  max mem: 3627
Test:  [1440/2050]  eta: 0:09:02    time: 0.9911  data: 0.0123  max mem: 3627
Test:  [1450/2050]  eta: 0:08:53    time: 0.9688  data: 0.0121  max mem: 3627
Test:  [1460/2050]  eta: 0:08:45    time: 0.9909  data: 0.0124  max mem: 3627
Test:  [1470/2050]  eta: 0:08:36    time: 0.9718  data: 0.0121  max mem: 3627
Test:  [1480/2050]  eta: 0:08:27    time: 0.9498  data: 0.0114  max mem: 3627
Test:  [1490/2050]  eta: 0:08:18    time: 0.9096  data: 0.0106  max mem: 3627
Test:  [1500/2050]  eta: 0:08:10    time: 0.9003  data: 0.0103  max mem: 3627
Test:  [1510/2050]  eta: 0:08:01    time: 0.9192  data: 0.0106  max mem: 3627
Test:  [1520/2050]  eta: 0:07:52    time: 0.9334  data: 0.0107  max mem: 3627
Test:  [1530/2050]  eta: 0:07:44    time: 1.0027  data: 0.0114  max mem: 3627
Test:  [1540/2050]  eta: 0:07:35    time: 1.0143  data: 0.0118  max mem: 3627
Test:  [1550/2050]  eta: 0:07:26    time: 0.9730  data: 0.0112  max mem: 3627
Test:  [1560/2050]  eta: 0:07:17    time: 0.9367  data: 0.0107  max mem: 3627
Test:  [1570/2050]  eta: 0:07:08    time: 0.8819  data: 0.0102  max mem: 3627
Test:  [1580/2050]  eta: 0:07:00    time: 0.9107  data: 0.0104  max mem: 3627
Test:  [1590/2050]  eta: 0:06:51    time: 0.9455  data: 0.0108  max mem: 3627
Test:  [1600/2050]  eta: 0:06:42    time: 0.9184  data: 0.0105  max mem: 3627
Test:  [1610/2050]  eta: 0:06:33    time: 0.9241  data: 0.0106  max mem: 3627
Test:  [1620/2050]  eta: 0:06:24    time: 0.9551  data: 0.0111  max mem: 3627
Test:  [1630/2050]  eta: 0:06:15    time: 0.9324  data: 0.0107  max mem: 3627
Test:  [1640/2050]  eta: 0:06:07    time: 0.8991  data: 0.0103  max mem: 3627
Test:  [1650/2050]  eta: 0:05:58    time: 0.8950  data: 0.0103  max mem: 3627
Test:  [1660/2050]  eta: 0:05:49    time: 0.9824  data: 0.0111  max mem: 3627
Test:  [1670/2050]  eta: 0:05:40    time: 1.0098  data: 0.0115  max mem: 3627
Test:  [1680/2050]  eta: 0:05:31    time: 0.9342  data: 0.0107  max mem: 3627
Test:  [1690/2050]  eta: 0:05:22    time: 0.9282  data: 0.0108  max mem: 3627
Test:  [1700/2050]  eta: 0:05:13    time: 0.8606  data: 0.0099  max mem: 3627
Test:  [1710/2050]  eta: 0:05:04    time: 0.8539  data: 0.0098  max mem: 3627
Test:  [1720/2050]  eta: 0:04:55    time: 0.9137  data: 0.0105  max mem: 3627
Test:  [1730/2050]  eta: 0:04:46    time: 0.9215  data: 0.0106  max mem: 3627
Test:  [1740/2050]  eta: 0:04:38    time: 0.9475  data: 0.0109  max mem: 3627
Test:  [1750/2050]  eta: 0:04:29    time: 0.9787  data: 0.0111  max mem: 3627
Test:  [1760/2050]  eta: 0:04:20    time: 0.9753  data: 0.0111  max mem: 3627
Test:  [1770/2050]  eta: 0:04:11    time: 0.8835  data: 0.0104  max mem: 3627
Test:  [1780/2050]  eta: 0:04:02    time: 0.8920  data: 0.0103  max mem: 3627
Test:  [1790/2050]  eta: 0:03:53    time: 0.9280  data: 0.0105  max mem: 3627
Test:  [1800/2050]  eta: 0:03:44    time: 0.8742  data: 0.0102  max mem: 3627
Test:  [1810/2050]  eta: 0:03:35    time: 0.8710  data: 0.0098  max mem: 3627
Test:  [1820/2050]  eta: 0:03:26    time: 0.8957  data: 0.0102  max mem: 3627
Test:  [1830/2050]  eta: 0:03:17    time: 0.9104  data: 0.0105  max mem: 3627
Test:  [1840/2050]  eta: 0:03:08    time: 0.8522  data: 0.0098  max mem: 3627
Test:  [1850/2050]  eta: 0:02:59    time: 0.8699  data: 0.0099  max mem: 3627
Test:  [1860/2050]  eta: 0:02:50    time: 0.9101  data: 0.0103  max mem: 3627
Test:  [1870/2050]  eta: 0:02:41    time: 0.8902  data: 0.0104  max mem: 3627
Test:  [1880/2050]  eta: 0:02:32    time: 0.8387  data: 0.0097  max mem: 3627
Test:  [1890/2050]  eta: 0:02:23    time: 0.8380  data: 0.0097  max mem: 3627
Test:  [1900/2050]  eta: 0:02:14    time: 0.9194  data: 0.0104  max mem: 3627
Test:  [1910/2050]  eta: 0:02:05    time: 0.8979  data: 0.0103  max mem: 3627
Test:  [1920/2050]  eta: 0:01:56    time: 0.9311  data: 0.0107  max mem: 3627
Test:  [1930/2050]  eta: 0:01:47    time: 0.9722  data: 0.0112  max mem: 3627
Test:  [1940/2050]  eta: 0:01:38    time: 0.8570  data: 0.0100  max mem: 3627
Test:  [1950/2050]  eta: 0:01:29    time: 0.8303  data: 0.0094  max mem: 3627
Test:  [1960/2050]  eta: 0:01:20    time: 0.9171  data: 0.0102  max mem: 3627
Test:  [1970/2050]  eta: 0:01:11    time: 0.9650  data: 0.0109  max mem: 3627
Test:  [1980/2050]  eta: 0:01:02    time: 0.9493  data: 0.0108  max mem: 3627
Test:  [1990/2050]  eta: 0:00:53    time: 0.9058  data: 0.0103  max mem: 3627
Test:  [2000/2050]  eta: 0:00:44    time: 0.9129  data: 0.0104  max mem: 3627
Test:  [2010/2050]  eta: 0:00:35    time: 0.9807  data: 0.0112  max mem: 3627
Test:  [2020/2050]  eta: 0:00:26    time: 0.9993  data: 0.0115  max mem: 3627
Test:  [2030/2050]  eta: 0:00:17    time: 0.9802  data: 0.0112  max mem: 3627
Test:  [2040/2050]  eta: 0:00:08    time: 0.9547  data: 0.0107  max mem: 3627
Test:  [2049/2050]  eta: 0:00:00    time: 0.8927  data: 0.0107  max mem: 3627
Test: Total time: 0:30:43 (0.8991 s / it)
aeroplane has 766 predictions.
bicycle has 1571 predictions.
bird has 1706 predictions.
boat has 2109 predictions.
bottle has 4282 predictions.
bus has 1512 predictions.
car has 7063 predictions.
cat has 917 predictions.
chair has 8215 predictions.
cow has 1488 predictions.
diningtable has 2269 predictions.
dog has 1424 predictions.
horse has 1601 predictions.
motorbike has 1314 predictions.
person has 33066 predictions.
pottedplant has 3185 predictions.
sheep has 1148 predictions.
sofa has 2943 predictions.
train has 947 predictions.
tvmonitor has 1987 predictions.
truck has 0 predictions.
traffic light has 0 predictions.
fire hydrant has 0 predictions.
stop sign has 0 predictions.
parking meter has 0 predictions.
bench has 0 predictions.
elephant has 0 predictions.
bear has 0 predictions.
zebra has 0 predictions.
giraffe has 0 predictions.
backpack has 0 predictions.
umbrella has 0 predictions.
handbag has 0 predictions.
tie has 0 predictions.
suitcase has 0 predictions.
microwave has 0 predictions.
oven has 0 predictions.
toaster has 0 predictions.
sink has 0 predictions.
refrigerator has 0 predictions.
frisbee has 0 predictions.
skis has 0 predictions.
snowboard has 0 predictions.
sports ball has 0 predictions.
kite has 0 predictions.
baseball bat has 0 predictions.
baseball glove has 0 predictions.
skateboard has 0 predictions.
surfboard has 0 predictions.
tennis racket has 0 predictions.
banana has 0 predictions.
apple has 0 predictions.
sandwich has 0 predictions.
orange has 0 predictions.
broccoli has 0 predictions.
carrot has 0 predictions.
hot dog has 0 predictions.
pizza has 0 predictions.
donut has 0 predictions.
cake has 0 predictions.
bed has 0 predictions.
toilet has 0 predictions.
laptop has 0 predictions.
mouse has 0 predictions.
remote has 0 predictions.
keyboard has 0 predictions.
cell phone has 0 predictions.
book has 0 predictions.
clock has 0 predictions.
vase has 0 predictions.
scissors has 0 predictions.
teddy bear has 0 predictions.
hair drier has 0 predictions.
toothbrush has 0 predictions.
wine glass has 0 predictions.
cup has 0 predictions.
fork has 0 predictions.
knife has 0 predictions.
spoon has 0 predictions.
bowl has 0 predictions.
unknown has 945087 predictions.
detection mAP50: 14.717399
detection mAP: 14.717399
---AP50---
Wilderness Impact: {0.1: {50: 0.007868207523973445}, 0.2: {50: 0.015517241379310345}, 0.3: {50: 0.02654926737464713}, 0.4: {50: 0.04953394521624228}, 0.5: {50: 0.048595669982445876}, 0.6: {50: 0.04772248268742742}, 0.7: {50: 0.05045703839122486}, 0.8: {50: 0.05719026078078249}, 0.9: {50: 0.06528745469863728}}
avg_precision: {0.1: {50: 0.021483583299554115}, 0.2: {50: 0.004944971978130014}, 0.3: {50: 0.004944971978130014}, 0.4: {50: 0.004944971978130014}, 0.5: {50: 0.004944971978130014}, 0.6: {50: 0.004944971978130014}, 0.7: {50: 0.004944971978130014}, 0.8: {50: 0.004944971978130014}, 0.9: {50: 0.004944971978130014}}
Absolute OSE (total_num_unk_det_as_known): {50: 5213.0}
total_num_unk 23320
AP50: ['79.0', '57.5', '64.2', '50.9', '30.2', '71.3', '55.5', '84.3', '26.6', '66.9', '22.9', '82.9', '75.4', '69.2', '49.7', '38.1', '72.0', '54.7', '79.7', '59.6', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '1.3']
Precisions50: ['39.2', '28.2', '31.6', '15.4', '20.7', '21.6', '30.9', '51.4', '17.0', '19.0', '21.1', '43.2', '23.7', '33.8', '34.8', '17.0', '24.2', '15.1', '35.3', '24.4', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.5']
Recall50: ['88.4', '67.1', '74.5', '65.4', '40.4', '81.7', '66.3', '91.8', '38.6', '87.0', '33.4', '91.6', '90.7', '78.8', '63.7', '56.9', '83.3', '75.4', '89.9', '75.6', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '19.5']
Current class AP50: tensor(59.5422)
Current class Precisions50: 27.390693722152957
Current class Recall50: 72.02219551857208
Known AP50: tensor(59.5422)
Known Precisions50: 27.390693722152957
Known Recall50: 72.02219551857208
Unknown AP50: tensor(1.2648)
Unknown Precisions50: 0.4816487794245397
Unknown Recall50: 19.519725557461406
aeroplane 79.013733
bicycle 57.509987
bird 64.249016
boat 50.922352
bottle 30.160067
bus 71.311073
car 55.526894
cat 84.330223
chair 26.637211
cow 66.948967
diningtable 22.910633
dog 82.874947
horse 75.417648
motorbike 69.208969
person 49.731781
pottedplant 38.079330
sheep 72.039894
sofa 54.669094
train 79.657646
tvmonitor 59.645035
truck 0.000000
traffic light 0.000000
fire hydrant 0.000000
stop sign 0.000000
parking meter 0.000000
bench 0.000000
elephant 0.000000
bear 0.000000
zebra 0.000000
giraffe 0.000000
backpack 0.000000
umbrella 0.000000
handbag 0.000000
tie 0.000000
suitcase 0.000000
microwave 0.000000
oven 0.000000
toaster 0.000000
sink 0.000000
refrigerator 0.000000
frisbee 0.000000
skis 0.000000
snowboard 0.000000
sports ball 0.000000
kite 0.000000
baseball bat 0.000000
baseball glove 0.000000
skateboard 0.000000
surfboard 0.000000
tennis racket 0.000000
banana 0.000000
apple 0.000000
sandwich 0.000000
orange 0.000000
broccoli 0.000000
carrot 0.000000
hot dog 0.000000
pizza 0.000000
donut 0.000000
cake 0.000000
bed 0.000000
toilet 0.000000
laptop 0.000000
mouse 0.000000
remote 0.000000
keyboard 0.000000
cell phone 0.000000
book 0.000000
clock 0.000000
vase 0.000000
scissors 0.000000
teddy bear 0.000000
hair drier 0.000000
toothbrush 0.000000
wine glass 0.000000
cup 0.000000
fork 0.000000
knife 0.000000
spoon 0.000000
bowl 0.000000
unknown 1.264824
+ PY_ARGS=
+ python -u main_open_world.py --output_dir exps/MOWODB/PROB/eval --dataset TOWOD --PREV_INTRODUCED_CLS 20 --CUR_INTRODUCED_CLS 20 --train_set owod_t1_train --test_set owod_all_task_test --epochs 191 --lr_drop 35 --model_type prob --obj_loss_coef 8e-4 --obj_temp 1.3 --pretrain exps/MOWODB/PROB/t2.pth --eval --wandb_project ''
{'OWDETR': ('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown'), 'TOWOD': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown'), 'VOC2007': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown')}
('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown')
| distributed init (rank 0): env://
git:
  sha: 5675c9e44c999734c9465041c4c1f191dda931f9, status: has uncommited changes, branch: main

Namespace(lr=0.0002, lr_backbone_names=['backbone.0'], lr_backbone=2e-05, lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, batch_size=5, weight_decay=0.0001, epochs=191, lr_drop=35, lr_drop_epochs=None, clip_max_norm=0.1, sgd=False, with_box_refine=False, two_stage=False, masks=False, backbone='dino_resnet50', frozen_weights=None, dilation=False, position_embedding='sine', position_embedding_scale=6.283185307179586, num_feature_levels=4, enc_layers=6, dec_layers=6, dim_feedforward=1024, hidden_dim=256, dropout=0.1, nheads=8, num_queries=100, dec_n_points=4, enc_n_points=4, aux_loss=True, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, cls_loss_coef=2, bbox_loss_coef=5, giou_loss_coef=2, focal_alpha=0.25, coco_panoptic_path=None, remove_difficult=False, output_dir='exps/MOWODB/PROB/eval', device='cuda', seed=42, resume='', start_epoch=0, eval=True, viz=False, eval_every=5, num_workers=3, cache_mode=False, PREV_INTRODUCED_CLS=20, CUR_INTRODUCED_CLS=20, unmatched_boxes=False, top_unk=5, featdim=1024, invalid_cls_logits=False, NC_branch=False, bbox_thresh=0.3, pretrain='exps/MOWODB/PROB/t2.pth', nc_loss_coef=2, train_set='owod_t1_train', test_set='owod_all_task_test', num_classes=81, nc_epoch=0, dataset='TOWOD', data_root='./data/OWOD', unk_conf_w=1.0, model_type='prob', wandb_name='', wandb_project='', obj_loss_coef=0.0008, obj_temp=1.3, freeze_prob_model=False, num_inst_per_class=50, exemplar_replay_selection=False, exemplar_replay_max_length=10000000000.0, exemplar_replay_dir='', exemplar_replay_prev_file='', exemplar_replay_cur_file='', exemplar_replay_random=False, rank=0, world_size=1, gpu=0, dist_url='env://', distributed=True, dist_backend='nccl')
Invalid class range: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]
DINO resnet50
/home/hannalukas/miniconda3/envs/prob/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/hannalukas/miniconda3/envs/prob/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
running with exemplar_replay_selection
DeformableDETR(
  (transformer): DeformableTransformer(
    (encoder): DeformableTransformerEncoder(
      (layers): ModuleList(
        (0): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (decoder): DeformableTransformerDecoder(
      (layers): ModuleList(
        (0): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (reference_points): Linear(in_features=256, out_features=2, bias=True)
  )
  (class_embed): ModuleList(
    (0): Linear(in_features=256, out_features=81, bias=True)
    (1): Linear(in_features=256, out_features=81, bias=True)
    (2): Linear(in_features=256, out_features=81, bias=True)
    (3): Linear(in_features=256, out_features=81, bias=True)
    (4): Linear(in_features=256, out_features=81, bias=True)
    (5): Linear(in_features=256, out_features=81, bias=True)
  )
  (bbox_embed): ModuleList(
    (0): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (1): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (2): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (3): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (4): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (5): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (prob_obj_head): ModuleList(
    (0): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (1): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (2): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (3): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (4): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (5): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (query_embed): Embedding(100, 512)
  (input_proj): ModuleList(
    (0): Sequential(
      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (1): Sequential(
      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (2): Sequential(
      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (3): Sequential(
      (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
  )
  (backbone): Joiner(
    (0): Backbone(
      (body): IntermediateLayerGetter(
        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
        (bn1): FrozenBatchNorm2d()
        (relu): ReLU(inplace=True)
        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
      )
    )
    (1): PositionEmbeddingSine()
  )
)
number of params: 39742295
TOWOD
owod_t1_train
owod_all_task_test
Dataset OWDetection
    Number of datapoints: 16551
    Root location: ./data/OWOD
    [['train'], Compose(
    <datasets.transforms.RandomHorizontalFlip object at 0x76a17574d7e0>
    <datasets.transforms.RandomSelect object at 0x76a17d4aef20>
    Compose(
    <datasets.transforms.ToTensor object at 0x76a175762fb0>
    <datasets.transforms.Normalize object at 0x76a24b580b50>
)
)]
Dataset OWDetection
    Number of datapoints: 10246
    Root location: ./data/OWOD
    [['test'], Compose(
    <datasets.transforms.RandomResize object at 0x76a17d4ae890>
    Compose(
    <datasets.transforms.ToTensor object at 0x76a17d4ae860>
    <datasets.transforms.Normalize object at 0x76a17d4aeaa0>
)
)]
Initialized from the pre-training model
<All keys matched successfully>
testing data details
81
80
('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator')
('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown')
/home/hannalukas/Documents/PROB/PROB_exp/models/position_encoding.py:49: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)
/home/hannalukas/miniconda3/envs/prob/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/hannalukas/Documents/PROB/PROB_exp/models/prob_deformable_detr.py:537: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  topk_boxes = topk_indexes // out_logits.shape[2]
Test:  [   0/2050]  eta: 1:15:11    time: 2.2006  data: 0.4265  max mem: 2694
Test:  [  10/2050]  eta: 0:35:52    time: 1.0552  data: 0.0476  max mem: 3324
Test:  [  20/2050]  eta: 0:33:23    time: 0.9265  data: 0.0100  max mem: 3324
Test:  [  30/2050]  eta: 0:31:50    time: 0.8854  data: 0.0098  max mem: 3324
Test:  [  40/2050]  eta: 0:31:15    time: 0.8765  data: 0.0096  max mem: 3324
Test:  [  50/2050]  eta: 0:30:24    time: 0.8602  data: 0.0097  max mem: 3324
Test:  [  60/2050]  eta: 0:29:55    time: 0.8395  data: 0.0095  max mem: 3324
Test:  [  70/2050]  eta: 0:29:14    time: 0.8197  data: 0.0092  max mem: 3324
Test:  [  80/2050]  eta: 0:28:53    time: 0.8112  data: 0.0089  max mem: 3324
Test:  [  90/2050]  eta: 0:29:07    time: 0.9111  data: 0.0099  max mem: 3324
Test:  [ 100/2050]  eta: 0:28:44    time: 0.9031  data: 0.0099  max mem: 3324
Test:  [ 110/2050]  eta: 0:28:40    time: 0.8643  data: 0.0097  max mem: 3324
Test:  [ 120/2050]  eta: 0:28:30    time: 0.8965  data: 0.0101  max mem: 3324
Test:  [ 130/2050]  eta: 0:28:10    time: 0.8465  data: 0.0096  max mem: 3324
Test:  [ 140/2050]  eta: 0:27:48    time: 0.7940  data: 0.0091  max mem: 3324
Test:  [ 150/2050]  eta: 0:27:36    time: 0.8159  data: 0.0092  max mem: 3324
Test:  [ 160/2050]  eta: 0:27:29    time: 0.8683  data: 0.0096  max mem: 3324
Test:  [ 170/2050]  eta: 0:27:27    time: 0.9082  data: 0.0101  max mem: 3324
Test:  [ 180/2050]  eta: 0:27:10    time: 0.8642  data: 0.0097  max mem: 3324
Test:  [ 190/2050]  eta: 0:27:01    time: 0.8331  data: 0.0096  max mem: 3324
Test:  [ 200/2050]  eta: 0:26:54    time: 0.8792  data: 0.0100  max mem: 3324
Test:  [ 210/2050]  eta: 0:26:46    time: 0.8881  data: 0.0099  max mem: 3324
Test:  [ 220/2050]  eta: 0:26:34    time: 0.8583  data: 0.0099  max mem: 3324
Test:  [ 230/2050]  eta: 0:26:26    time: 0.8525  data: 0.0097  max mem: 3324
Test:  [ 240/2050]  eta: 0:26:19    time: 0.8862  data: 0.0097  max mem: 3324
Test:  [ 250/2050]  eta: 0:26:13    time: 0.9062  data: 0.0102  max mem: 3324
Test:  [ 260/2050]  eta: 0:26:05    time: 0.9011  data: 0.0103  max mem: 3324
Test:  [ 270/2050]  eta: 0:25:54    time: 0.8630  data: 0.0097  max mem: 3324
Test:  [ 280/2050]  eta: 0:25:44    time: 0.8415  data: 0.0097  max mem: 3324
Test:  [ 290/2050]  eta: 0:25:38    time: 0.8836  data: 0.0098  max mem: 3324
Test:  [ 300/2050]  eta: 0:25:34    time: 0.9384  data: 0.0105  max mem: 3324
Test:  [ 310/2050]  eta: 0:25:24    time: 0.9035  data: 0.0104  max mem: 3324
Test:  [ 320/2050]  eta: 0:25:16    time: 0.8725  data: 0.0099  max mem: 3324
Test:  [ 330/2050]  eta: 0:25:09    time: 0.9056  data: 0.0103  max mem: 3324
Test:  [ 340/2050]  eta: 0:25:02    time: 0.9102  data: 0.0102  max mem: 3324
Test:  [ 350/2050]  eta: 0:24:49    time: 0.8536  data: 0.0098  max mem: 3324
Test:  [ 360/2050]  eta: 0:24:41    time: 0.8489  data: 0.0096  max mem: 3324
Test:  [ 370/2050]  eta: 0:24:29    time: 0.8466  data: 0.0092  max mem: 3324
Test:  [ 380/2050]  eta: 0:24:17    time: 0.8025  data: 0.0087  max mem: 3324
Test:  [ 390/2050]  eta: 0:24:11    time: 0.8644  data: 0.0093  max mem: 3324
Test:  [ 400/2050]  eta: 0:24:02    time: 0.9054  data: 0.0098  max mem: 3324
Test:  [ 410/2050]  eta: 0:23:54    time: 0.8863  data: 0.0094  max mem: 3324
Test:  [ 420/2050]  eta: 0:23:47    time: 0.8985  data: 0.0093  max mem: 3324
Test:  [ 430/2050]  eta: 0:23:37    time: 0.8847  data: 0.0092  max mem: 3324
Test:  [ 440/2050]  eta: 0:23:30    time: 0.8791  data: 0.0094  max mem: 3324
Test:  [ 450/2050]  eta: 0:23:20    time: 0.8721  data: 0.0094  max mem: 3324
Test:  [ 460/2050]  eta: 0:23:10    time: 0.8500  data: 0.0091  max mem: 3324
Test:  [ 470/2050]  eta: 0:23:00    time: 0.8380  data: 0.0089  max mem: 3324
Test:  [ 480/2050]  eta: 0:22:53    time: 0.8827  data: 0.0093  max mem: 3324
Test:  [ 490/2050]  eta: 0:22:48    time: 0.9574  data: 0.0101  max mem: 3324
Test:  [ 500/2050]  eta: 0:22:39    time: 0.9333  data: 0.0098  max mem: 3324
Test:  [ 510/2050]  eta: 0:22:28    time: 0.8393  data: 0.0089  max mem: 3324
Test:  [ 520/2050]  eta: 0:22:17    time: 0.7928  data: 0.0085  max mem: 3324
Test:  [ 530/2050]  eta: 0:22:07    time: 0.8172  data: 0.0088  max mem: 3324
Test:  [ 540/2050]  eta: 0:22:00    time: 0.8890  data: 0.0094  max mem: 3324
Test:  [ 550/2050]  eta: 0:21:52    time: 0.9191  data: 0.0096  max mem: 3324
Test:  [ 560/2050]  eta: 0:21:45    time: 0.9133  data: 0.0097  max mem: 3627
Test:  [ 570/2050]  eta: 0:21:36    time: 0.9076  data: 0.0096  max mem: 3627
Test:  [ 580/2050]  eta: 0:21:26    time: 0.8545  data: 0.0089  max mem: 3627
Test:  [ 590/2050]  eta: 0:21:17    time: 0.8510  data: 0.0091  max mem: 3627
Test:  [ 600/2050]  eta: 0:21:08    time: 0.8615  data: 0.0093  max mem: 3627
Test:  [ 610/2050]  eta: 0:21:00    time: 0.8803  data: 0.0092  max mem: 3627
Test:  [ 620/2050]  eta: 0:20:52    time: 0.9092  data: 0.0096  max mem: 3627
Test:  [ 630/2050]  eta: 0:20:43    time: 0.8745  data: 0.0094  max mem: 3627
Test:  [ 640/2050]  eta: 0:20:34    time: 0.8599  data: 0.0092  max mem: 3627
Test:  [ 650/2050]  eta: 0:20:26    time: 0.9017  data: 0.0096  max mem: 3627
Test:  [ 660/2050]  eta: 0:20:19    time: 0.9357  data: 0.0098  max mem: 3627
Test:  [ 670/2050]  eta: 0:20:09    time: 0.8871  data: 0.0092  max mem: 3627
Test:  [ 680/2050]  eta: 0:20:01    time: 0.8600  data: 0.0093  max mem: 3627
Test:  [ 690/2050]  eta: 0:19:51    time: 0.8694  data: 0.0091  max mem: 3627
Test:  [ 700/2050]  eta: 0:19:43    time: 0.8737  data: 0.0091  max mem: 3627
Test:  [ 710/2050]  eta: 0:19:35    time: 0.9019  data: 0.0098  max mem: 3627
Test:  [ 720/2050]  eta: 0:19:26    time: 0.8940  data: 0.0096  max mem: 3627
Test:  [ 730/2050]  eta: 0:19:18    time: 0.8900  data: 0.0093  max mem: 3627
Test:  [ 740/2050]  eta: 0:19:09    time: 0.8815  data: 0.0093  max mem: 3627
Test:  [ 750/2050]  eta: 0:18:59    time: 0.8362  data: 0.0090  max mem: 3627
Test:  [ 760/2050]  eta: 0:18:52    time: 0.8988  data: 0.0094  max mem: 3627
Test:  [ 770/2050]  eta: 0:18:43    time: 0.9405  data: 0.0099  max mem: 3627
Test:  [ 780/2050]  eta: 0:18:34    time: 0.8683  data: 0.0093  max mem: 3627
Test:  [ 790/2050]  eta: 0:18:25    time: 0.8434  data: 0.0089  max mem: 3627
Test:  [ 800/2050]  eta: 0:18:17    time: 0.8893  data: 0.0094  max mem: 3627
Test:  [ 810/2050]  eta: 0:18:08    time: 0.9113  data: 0.0097  max mem: 3627
Test:  [ 820/2050]  eta: 0:17:59    time: 0.8528  data: 0.0092  max mem: 3627
Test:  [ 830/2050]  eta: 0:17:50    time: 0.8550  data: 0.0092  max mem: 3627
Test:  [ 840/2050]  eta: 0:17:40    time: 0.8475  data: 0.0089  max mem: 3627
Test:  [ 850/2050]  eta: 0:17:32    time: 0.8659  data: 0.0090  max mem: 3627
Test:  [ 860/2050]  eta: 0:17:24    time: 0.9415  data: 0.0102  max mem: 3627
Test:  [ 870/2050]  eta: 0:17:16    time: 0.9176  data: 0.0099  max mem: 3627
Test:  [ 880/2050]  eta: 0:17:05    time: 0.8233  data: 0.0086  max mem: 3627
Test:  [ 890/2050]  eta: 0:16:57    time: 0.8357  data: 0.0087  max mem: 3627
Test:  [ 900/2050]  eta: 0:16:48    time: 0.8873  data: 0.0095  max mem: 3627
Test:  [ 910/2050]  eta: 0:16:39    time: 0.8440  data: 0.0091  max mem: 3627
Test:  [ 920/2050]  eta: 0:16:29    time: 0.8205  data: 0.0087  max mem: 3627
Test:  [ 930/2050]  eta: 0:16:20    time: 0.8479  data: 0.0090  max mem: 3627
Test:  [ 940/2050]  eta: 0:16:12    time: 0.8932  data: 0.0094  max mem: 3627
Test:  [ 950/2050]  eta: 0:16:03    time: 0.8941  data: 0.0092  max mem: 3627
Test:  [ 960/2050]  eta: 0:15:54    time: 0.8556  data: 0.0092  max mem: 3627
Test:  [ 970/2050]  eta: 0:15:45    time: 0.8422  data: 0.0093  max mem: 3627
Test:  [ 980/2050]  eta: 0:15:36    time: 0.8290  data: 0.0090  max mem: 3627
Test:  [ 990/2050]  eta: 0:15:27    time: 0.8408  data: 0.0088  max mem: 3627
Test:  [1000/2050]  eta: 0:15:19    time: 0.9126  data: 0.0096  max mem: 3627
Test:  [1010/2050]  eta: 0:15:11    time: 0.9755  data: 0.0104  max mem: 3627
Test:  [1020/2050]  eta: 0:15:03    time: 0.9540  data: 0.0103  max mem: 3627
Test:  [1030/2050]  eta: 0:14:55    time: 0.9274  data: 0.0101  max mem: 3627
Test:  [1040/2050]  eta: 0:14:47    time: 0.9537  data: 0.0100  max mem: 3627
Test:  [1050/2050]  eta: 0:14:39    time: 0.9908  data: 0.0103  max mem: 3627
Test:  [1060/2050]  eta: 0:14:31    time: 0.9474  data: 0.0103  max mem: 3627
Test:  [1070/2050]  eta: 0:14:21    time: 0.8496  data: 0.0093  max mem: 3627
Test:  [1080/2050]  eta: 0:14:12    time: 0.8484  data: 0.0091  max mem: 3627
Test:  [1090/2050]  eta: 0:14:03    time: 0.8704  data: 0.0093  max mem: 3627
Test:  [1100/2050]  eta: 0:13:56    time: 0.9382  data: 0.0097  max mem: 3627
Test:  [1110/2050]  eta: 0:13:47    time: 0.9261  data: 0.0096  max mem: 3627
Test:  [1120/2050]  eta: 0:13:39    time: 0.9208  data: 0.0098  max mem: 3627
Test:  [1130/2050]  eta: 0:13:30    time: 0.9178  data: 0.0099  max mem: 3627
Test:  [1140/2050]  eta: 0:13:20    time: 0.8201  data: 0.0088  max mem: 3627
Test:  [1150/2050]  eta: 0:13:11    time: 0.8325  data: 0.0087  max mem: 3627
Test:  [1160/2050]  eta: 0:13:03    time: 0.9005  data: 0.0096  max mem: 3627
Test:  [1170/2050]  eta: 0:12:54    time: 0.9204  data: 0.0098  max mem: 3627
Test:  [1180/2050]  eta: 0:12:46    time: 0.9315  data: 0.0097  max mem: 3627
Test:  [1190/2050]  eta: 0:12:37    time: 0.9183  data: 0.0097  max mem: 3627
Test:  [1200/2050]  eta: 0:12:29    time: 0.9287  data: 0.0098  max mem: 3627
Test:  [1210/2050]  eta: 0:12:21    time: 0.9812  data: 0.0102  max mem: 3627
Test:  [1220/2050]  eta: 0:12:13    time: 0.9878  data: 0.0106  max mem: 3627
Test:  [1230/2050]  eta: 0:12:05    time: 1.0249  data: 0.0108  max mem: 3627
Test:  [1240/2050]  eta: 0:11:57    time: 1.0070  data: 0.0105  max mem: 3627
Test:  [1250/2050]  eta: 0:11:48    time: 0.9324  data: 0.0101  max mem: 3627
Test:  [1260/2050]  eta: 0:11:40    time: 0.9217  data: 0.0098  max mem: 3627
Test:  [1270/2050]  eta: 0:11:31    time: 0.8943  data: 0.0095  max mem: 3627
Test:  [1280/2050]  eta: 0:11:22    time: 0.8871  data: 0.0094  max mem: 3627
Test:  [1290/2050]  eta: 0:11:13    time: 0.8896  data: 0.0095  max mem: 3627
Test:  [1300/2050]  eta: 0:11:04    time: 0.8587  data: 0.0091  max mem: 3627
Test:  [1310/2050]  eta: 0:10:55    time: 0.8784  data: 0.0092  max mem: 3627
Test:  [1320/2050]  eta: 0:10:46    time: 0.8891  data: 0.0096  max mem: 3627
Test:  [1330/2050]  eta: 0:10:38    time: 0.9326  data: 0.0098  max mem: 3627
Test:  [1340/2050]  eta: 0:10:29    time: 0.9223  data: 0.0098  max mem: 3627
Test:  [1350/2050]  eta: 0:10:20    time: 0.9118  data: 0.0097  max mem: 3627
Test:  [1360/2050]  eta: 0:10:11    time: 0.8984  data: 0.0094  max mem: 3627
Test:  [1370/2050]  eta: 0:10:02    time: 0.8399  data: 0.0090  max mem: 3627
Test:  [1380/2050]  eta: 0:09:54    time: 0.8943  data: 0.0095  max mem: 3627
Test:  [1390/2050]  eta: 0:09:45    time: 0.9112  data: 0.0098  max mem: 3627
Test:  [1400/2050]  eta: 0:09:36    time: 0.8579  data: 0.0092  max mem: 3627
Test:  [1410/2050]  eta: 0:09:27    time: 0.8758  data: 0.0091  max mem: 3627
Test:  [1420/2050]  eta: 0:09:18    time: 0.9032  data: 0.0093  max mem: 3627
Test:  [1430/2050]  eta: 0:09:10    time: 0.9658  data: 0.0100  max mem: 3627
Test:  [1440/2050]  eta: 0:09:01    time: 0.9884  data: 0.0104  max mem: 3627
Test:  [1450/2050]  eta: 0:08:53    time: 0.9659  data: 0.0102  max mem: 3627
Test:  [1460/2050]  eta: 0:08:44    time: 0.9887  data: 0.0105  max mem: 3627
Test:  [1470/2050]  eta: 0:08:36    time: 0.9698  data: 0.0103  max mem: 3627
Test:  [1480/2050]  eta: 0:08:27    time: 0.9485  data: 0.0101  max mem: 3627
Test:  [1490/2050]  eta: 0:08:18    time: 0.9086  data: 0.0096  max mem: 3627
Test:  [1500/2050]  eta: 0:08:09    time: 0.9002  data: 0.0094  max mem: 3627
Test:  [1510/2050]  eta: 0:08:00    time: 0.9191  data: 0.0097  max mem: 3627
Test:  [1520/2050]  eta: 0:07:52    time: 0.9334  data: 0.0098  max mem: 3627
Test:  [1530/2050]  eta: 0:07:43    time: 1.0028  data: 0.0104  max mem: 3627
Test:  [1540/2050]  eta: 0:07:35    time: 1.0136  data: 0.0109  max mem: 3627
Test:  [1550/2050]  eta: 0:07:26    time: 0.9722  data: 0.0103  max mem: 3627
Test:  [1560/2050]  eta: 0:07:17    time: 0.9364  data: 0.0099  max mem: 3627
Test:  [1570/2050]  eta: 0:07:08    time: 0.8817  data: 0.0094  max mem: 3627
Test:  [1580/2050]  eta: 0:06:59    time: 0.9104  data: 0.0095  max mem: 3627
Test:  [1590/2050]  eta: 0:06:50    time: 0.9451  data: 0.0100  max mem: 3627
Test:  [1600/2050]  eta: 0:06:42    time: 0.9181  data: 0.0098  max mem: 3627
Test:  [1610/2050]  eta: 0:06:33    time: 0.9240  data: 0.0101  max mem: 3627
Test:  [1620/2050]  eta: 0:06:24    time: 0.9560  data: 0.0113  max mem: 3627
Test:  [1630/2050]  eta: 0:06:15    time: 0.9348  data: 0.0115  max mem: 3627
Test:  [1640/2050]  eta: 0:06:06    time: 0.9004  data: 0.0107  max mem: 3627
Test:  [1650/2050]  eta: 0:05:57    time: 0.8951  data: 0.0100  max mem: 3627
Test:  [1660/2050]  eta: 0:05:49    time: 0.9829  data: 0.0108  max mem: 3627
Test:  [1670/2050]  eta: 0:05:40    time: 1.0102  data: 0.0112  max mem: 3627
Test:  [1680/2050]  eta: 0:05:31    time: 0.9349  data: 0.0105  max mem: 3627
Test:  [1690/2050]  eta: 0:05:22    time: 0.9290  data: 0.0105  max mem: 3627
Test:  [1700/2050]  eta: 0:05:13    time: 0.8611  data: 0.0097  max mem: 3627
Test:  [1710/2050]  eta: 0:05:04    time: 0.8541  data: 0.0096  max mem: 3627
Test:  [1720/2050]  eta: 0:04:55    time: 0.9139  data: 0.0103  max mem: 3627
Test:  [1730/2050]  eta: 0:04:46    time: 0.9214  data: 0.0103  max mem: 3627
Test:  [1740/2050]  eta: 0:04:37    time: 0.9475  data: 0.0105  max mem: 3627
Test:  [1750/2050]  eta: 0:04:29    time: 0.9793  data: 0.0109  max mem: 3627
Test:  [1760/2050]  eta: 0:04:20    time: 0.9756  data: 0.0106  max mem: 3627
Test:  [1770/2050]  eta: 0:04:11    time: 0.8836  data: 0.0099  max mem: 3627
Test:  [1780/2050]  eta: 0:04:02    time: 0.8921  data: 0.0099  max mem: 3627
Test:  [1790/2050]  eta: 0:03:53    time: 0.9278  data: 0.0102  max mem: 3627
Test:  [1800/2050]  eta: 0:03:44    time: 0.8738  data: 0.0099  max mem: 3627
Test:  [1810/2050]  eta: 0:03:35    time: 0.8709  data: 0.0096  max mem: 3627
Test:  [1820/2050]  eta: 0:03:26    time: 0.8958  data: 0.0100  max mem: 3627
Test:  [1830/2050]  eta: 0:03:17    time: 0.9109  data: 0.0102  max mem: 3627
Test:  [1840/2050]  eta: 0:03:08    time: 0.8524  data: 0.0094  max mem: 3627
Test:  [1850/2050]  eta: 0:02:59    time: 0.8700  data: 0.0096  max mem: 3627
Test:  [1860/2050]  eta: 0:02:50    time: 0.9104  data: 0.0101  max mem: 3627
Test:  [1870/2050]  eta: 0:02:41    time: 0.8903  data: 0.0100  max mem: 3627
Test:  [1880/2050]  eta: 0:02:32    time: 0.8388  data: 0.0092  max mem: 3627
Test:  [1890/2050]  eta: 0:02:23    time: 0.8383  data: 0.0094  max mem: 3627
Test:  [1900/2050]  eta: 0:02:14    time: 0.9197  data: 0.0102  max mem: 3627
Test:  [1910/2050]  eta: 0:02:05    time: 0.8979  data: 0.0099  max mem: 3627
Test:  [1920/2050]  eta: 0:01:56    time: 0.9316  data: 0.0104  max mem: 3627
Test:  [1930/2050]  eta: 0:01:47    time: 0.9727  data: 0.0109  max mem: 3627
Test:  [1940/2050]  eta: 0:01:38    time: 0.8574  data: 0.0097  max mem: 3627
Test:  [1950/2050]  eta: 0:01:29    time: 0.8306  data: 0.0092  max mem: 3627
Test:  [1960/2050]  eta: 0:01:20    time: 0.9173  data: 0.0100  max mem: 3627
Test:  [1970/2050]  eta: 0:01:11    time: 0.9656  data: 0.0107  max mem: 3627
Test:  [1980/2050]  eta: 0:01:02    time: 0.9500  data: 0.0106  max mem: 3627
Test:  [1990/2050]  eta: 0:00:53    time: 0.9062  data: 0.0101  max mem: 3627
Test:  [2000/2050]  eta: 0:00:44    time: 0.9131  data: 0.0101  max mem: 3627
Test:  [2010/2050]  eta: 0:00:35    time: 0.9807  data: 0.0108  max mem: 3627
Test:  [2020/2050]  eta: 0:00:26    time: 0.9991  data: 0.0112  max mem: 3627
Test:  [2030/2050]  eta: 0:00:17    time: 0.9809  data: 0.0110  max mem: 3627
Test:  [2040/2050]  eta: 0:00:08    time: 0.9553  data: 0.0105  max mem: 3627
Test:  [2049/2050]  eta: 0:00:00    time: 0.8933  data: 0.0104  max mem: 3627
Test: Total time: 0:30:42 (0.8986 s / it)
aeroplane has 1095 predictions.
bicycle has 2436 predictions.
bird has 2555 predictions.
boat has 2519 predictions.
bottle has 8801 predictions.
bus has 1364 predictions.
car has 11031 predictions.
cat has 1565 predictions.
chair has 13540 predictions.
cow has 1334 predictions.
diningtable has 6095 predictions.
dog has 1840 predictions.
horse has 1106 predictions.
motorbike has 1977 predictions.
person has 44088 predictions.
pottedplant has 4177 predictions.
sheep has 1074 predictions.
sofa has 3861 predictions.
train has 827 predictions.
tvmonitor has 2269 predictions.
truck has 4610 predictions.
traffic light has 2129 predictions.
fire hydrant has 1274 predictions.
stop sign has 857 predictions.
parking meter has 1488 predictions.
bench has 8468 predictions.
elephant has 2008 predictions.
bear has 1644 predictions.
zebra has 1390 predictions.
giraffe has 1567 predictions.
backpack has 4779 predictions.
umbrella has 3152 predictions.
handbag has 6322 predictions.
tie has 3144 predictions.
suitcase has 2791 predictions.
microwave has 1830 predictions.
oven has 4399 predictions.
toaster has 2621 predictions.
sink has 5878 predictions.
refrigerator has 3343 predictions.
frisbee has 0 predictions.
skis has 0 predictions.
snowboard has 0 predictions.
sports ball has 0 predictions.
kite has 0 predictions.
baseball bat has 0 predictions.
baseball glove has 0 predictions.
skateboard has 0 predictions.
surfboard has 0 predictions.
tennis racket has 0 predictions.
banana has 0 predictions.
apple has 0 predictions.
sandwich has 0 predictions.
orange has 0 predictions.
broccoli has 0 predictions.
carrot has 0 predictions.
hot dog has 0 predictions.
pizza has 0 predictions.
donut has 0 predictions.
cake has 0 predictions.
bed has 0 predictions.
toilet has 0 predictions.
laptop has 0 predictions.
mouse has 0 predictions.
remote has 0 predictions.
keyboard has 0 predictions.
cell phone has 0 predictions.
book has 0 predictions.
clock has 0 predictions.
vase has 0 predictions.
scissors has 0 predictions.
teddy bear has 0 predictions.
hair drier has 0 predictions.
toothbrush has 0 predictions.
wine glass has 0 predictions.
cup has 0 predictions.
fork has 0 predictions.
knife has 0 predictions.
spoon has 0 predictions.
bowl has 0 predictions.
unknown has 847352 predictions.
detection mAP50: 21.690536
detection mAP: 21.690536
---AP50---
Wilderness Impact: {0.1: {50: 0.007177033492822967}, 0.2: {50: 0.022140387119953295}, 0.3: {50: 0.03034772936112589}, 0.4: {50: 0.04215581643543223}, 0.5: {50: 0.04420625288640196}, 0.6: {50: 0.03945422109066134}, 0.7: {50: 0.03437623063239228}, 0.8: {50: 0.03459910711981626}, 0.9: {50: 0.034623921085080146}}
avg_precision: {0.1: {50: 0.01246682575436562}, 0.2: {50: 0.0034633011184194906}, 0.3: {50: 0.0034633011184194906}, 0.4: {50: 0.0034633011184194906}, 0.5: {50: 0.0034633011184194906}, 0.6: {50: 0.0034633011184194906}, 0.7: {50: 0.0034633011184194906}, 0.8: {50: 0.0034633011184194906}, 0.9: {50: 0.0034633011184194906}}
Absolute OSE (total_num_unk_det_as_known): {50: 6430.0}
total_num_unk 16768
AP50: ['69.3', '54.6', '59.5', '46.4', '27.9', '66.7', '52.3', '75.8', '26.1', '65.1', '21.9', '72.0', '79.8', '62.7', '49.4', '34.3', '65.3', '51.0', '78.8', '53.3', '22.3', '22.4', '57.8', '55.0', '37.8', '12.2', '66.9', '60.5', '80.2', '75.9', '10.4', '25.2', '10.2', '16.1', '20.5', '20.2', '15.1', '3.2', '11.9', '20.4', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.4']
Precisions50: ['24.8', '17.9', '20.0', '12.4', '10.6', '22.8', '20.0', '29.3', '10.7', '20.8', '9.0', '32.1', '33.4', '20.4', '26.2', '12.6', '24.4', '11.3', '39.2', '20.3', '5.9', '14.0', '6.0', '6.5', '2.7', '2.1', '11.5', '4.0', '17.8', '13.2', '2.3', '6.8', '2.1', '3.6', '5.4', '2.5', '2.3', '0.2', '2.4', '2.7', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.3']
Recall50: ['80.3', '66.5', '72.2', '63.1', '42.6', '77.6', '67.9', '89.2', '41.1', '85.0', '39.2', '88.1', '87.3', '73.1', '64.0', '55.3', '78.0', '76.0', '86.8', '71.9', '57.8', '40.9', '70.4', '72.7', '63.5', '28.8', '89.2', '89.0', '91.5', '88.5', '19.8', '41.7', '17.4', '30.8', '41.9', '42.1', '33.9', '29.4', '33.6', '42.5', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '17.4']
Prev class AP50: tensor(55.6182)
Prev class Precisions50: 20.892646506638407
Prev class Recall50: 70.26226548675318
Current class AP50: tensor(32.2072)
Current class Precisions50: 5.702121211687571
Current class Recall50: 51.26797745209829
Known AP50: tensor(43.9127)
Known Precisions50: 13.297383859162991
Known Recall50: 60.76512146942573
Unknown AP50: tensor(0.4255)
Unknown Precisions50: 0.3442489071837914
Unknown Recall50: 17.396230916030532
aeroplane 69.304016
bicycle 54.644966
bird 59.510319
boat 46.429970
bottle 27.930298
bus 66.710075
car 52.301662
cat 75.784874
chair 26.060911
cow 65.149818
diningtable 21.945019
dog 72.030357
horse 79.769691
motorbike 62.681519
person 49.416065
pottedplant 34.330196
sheep 65.256226
sofa 51.009804
train 78.844658
tvmonitor 53.254147
truck 22.258488
traffic light 22.360382
fire hydrant 57.826729
stop sign 55.015732
parking meter 37.784912
bench 12.243672
elephant 66.939964
bear 60.502304
zebra 80.237663
giraffe 75.894836
backpack 10.366826
umbrella 25.173700
handbag 10.221835
tie 16.124580
suitcase 20.520321
microwave 20.158497
oven 15.077861
toaster 3.163017
sink 11.864483
refrigerator 20.407501
frisbee 0.000000
skis 0.000000
snowboard 0.000000
sports ball 0.000000
kite 0.000000
baseball bat 0.000000
baseball glove 0.000000
skateboard 0.000000
surfboard 0.000000
tennis racket 0.000000
banana 0.000000
apple 0.000000
sandwich 0.000000
orange 0.000000
broccoli 0.000000
carrot 0.000000
hot dog 0.000000
pizza 0.000000
donut 0.000000
cake 0.000000
bed 0.000000
toilet 0.000000
laptop 0.000000
mouse 0.000000
remote 0.000000
keyboard 0.000000
cell phone 0.000000
book 0.000000
clock 0.000000
vase 0.000000
scissors 0.000000
teddy bear 0.000000
hair drier 0.000000
toothbrush 0.000000
wine glass 0.000000
cup 0.000000
fork 0.000000
knife 0.000000
spoon 0.000000
bowl 0.000000
unknown 0.425519
+ PY_ARGS=
+ python -u main_open_world.py --output_dir exps/MOWODB/PROB/eval --dataset TOWOD --PREV_INTRODUCED_CLS 40 --CUR_INTRODUCED_CLS 20 --train_set owod_t1_train --test_set owod_all_task_test --epochs 191 --lr_drop 35 --model_type prob --obj_loss_coef 8e-4 --obj_temp 1.3 --pretrain exps/MOWODB/PROB/t3.pth --eval --wandb_project ''
{'OWDETR': ('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown'), 'TOWOD': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown'), 'VOC2007': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown')}
('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown')
| distributed init (rank 0): env://
git:
  sha: 5675c9e44c999734c9465041c4c1f191dda931f9, status: has uncommited changes, branch: main

Namespace(lr=0.0002, lr_backbone_names=['backbone.0'], lr_backbone=2e-05, lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, batch_size=5, weight_decay=0.0001, epochs=191, lr_drop=35, lr_drop_epochs=None, clip_max_norm=0.1, sgd=False, with_box_refine=False, two_stage=False, masks=False, backbone='dino_resnet50', frozen_weights=None, dilation=False, position_embedding='sine', position_embedding_scale=6.283185307179586, num_feature_levels=4, enc_layers=6, dec_layers=6, dim_feedforward=1024, hidden_dim=256, dropout=0.1, nheads=8, num_queries=100, dec_n_points=4, enc_n_points=4, aux_loss=True, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, cls_loss_coef=2, bbox_loss_coef=5, giou_loss_coef=2, focal_alpha=0.25, coco_panoptic_path=None, remove_difficult=False, output_dir='exps/MOWODB/PROB/eval', device='cuda', seed=42, resume='', start_epoch=0, eval=True, viz=False, eval_every=5, num_workers=3, cache_mode=False, PREV_INTRODUCED_CLS=40, CUR_INTRODUCED_CLS=20, unmatched_boxes=False, top_unk=5, featdim=1024, invalid_cls_logits=False, NC_branch=False, bbox_thresh=0.3, pretrain='exps/MOWODB/PROB/t3.pth', nc_loss_coef=2, train_set='owod_t1_train', test_set='owod_all_task_test', num_classes=81, nc_epoch=0, dataset='TOWOD', data_root='./data/OWOD', unk_conf_w=1.0, model_type='prob', wandb_name='', wandb_project='', obj_loss_coef=0.0008, obj_temp=1.3, freeze_prob_model=False, num_inst_per_class=50, exemplar_replay_selection=False, exemplar_replay_max_length=10000000000.0, exemplar_replay_dir='', exemplar_replay_prev_file='', exemplar_replay_cur_file='', exemplar_replay_random=False, rank=0, world_size=1, gpu=0, dist_url='env://', distributed=True, dist_backend='nccl')
Invalid class range: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]
DINO resnet50
/home/hannalukas/miniconda3/envs/prob/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/hannalukas/miniconda3/envs/prob/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
running with exemplar_replay_selection
DeformableDETR(
  (transformer): DeformableTransformer(
    (encoder): DeformableTransformerEncoder(
      (layers): ModuleList(
        (0): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (decoder): DeformableTransformerDecoder(
      (layers): ModuleList(
        (0): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (reference_points): Linear(in_features=256, out_features=2, bias=True)
  )
  (class_embed): ModuleList(
    (0): Linear(in_features=256, out_features=81, bias=True)
    (1): Linear(in_features=256, out_features=81, bias=True)
    (2): Linear(in_features=256, out_features=81, bias=True)
    (3): Linear(in_features=256, out_features=81, bias=True)
    (4): Linear(in_features=256, out_features=81, bias=True)
    (5): Linear(in_features=256, out_features=81, bias=True)
  )
  (bbox_embed): ModuleList(
    (0): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (1): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (2): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (3): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (4): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (5): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (prob_obj_head): ModuleList(
    (0): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (1): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (2): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (3): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (4): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (5): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (query_embed): Embedding(100, 512)
  (input_proj): ModuleList(
    (0): Sequential(
      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (1): Sequential(
      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (2): Sequential(
      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (3): Sequential(
      (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
  )
  (backbone): Joiner(
    (0): Backbone(
      (body): IntermediateLayerGetter(
        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
        (bn1): FrozenBatchNorm2d()
        (relu): ReLU(inplace=True)
        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
      )
    )
    (1): PositionEmbeddingSine()
  )
)
number of params: 39742295
TOWOD
owod_t1_train
owod_all_task_test
Dataset OWDetection
    Number of datapoints: 16551
    Root location: ./data/OWOD
    [['train'], Compose(
    <datasets.transforms.RandomHorizontalFlip object at 0x740cf44e5990>
    <datasets.transforms.RandomSelect object at 0x740cf4476fe0>
    Compose(
    <datasets.transforms.ToTensor object at 0x740cf44fafb0>
    <datasets.transforms.Normalize object at 0x740db9178b50>
)
)]
Dataset OWDetection
    Number of datapoints: 10246
    Root location: ./data/OWOD
    [['test'], Compose(
    <datasets.transforms.RandomResize object at 0x740cf4476c50>
    Compose(
    <datasets.transforms.ToTensor object at 0x740cf4476bf0>
    <datasets.transforms.Normalize object at 0x740cf4476cb0>
)
)]
Initialized from the pre-training model
<All keys matched successfully>
testing data details
81
80
('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake')
('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown')
/home/hannalukas/Documents/PROB/PROB_exp/models/position_encoding.py:49: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)
/home/hannalukas/miniconda3/envs/prob/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/hannalukas/Documents/PROB/PROB_exp/models/prob_deformable_detr.py:537: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  topk_boxes = topk_indexes // out_logits.shape[2]
Test:  [   0/2050]  eta: 1:16:18    time: 2.2333  data: 0.4295  max mem: 2694
Test:  [  10/2050]  eta: 0:35:58    time: 1.0582  data: 0.0481  max mem: 3324
Test:  [  20/2050]  eta: 0:33:25    time: 0.9256  data: 0.0099  max mem: 3324
Test:  [  30/2050]  eta: 0:31:49    time: 0.8834  data: 0.0096  max mem: 3324
Test:  [  40/2050]  eta: 0:31:14    time: 0.8747  data: 0.0093  max mem: 3324
Test:  [  50/2050]  eta: 0:30:22    time: 0.8588  data: 0.0092  max mem: 3324
Test:  [  60/2050]  eta: 0:29:54    time: 0.8383  data: 0.0090  max mem: 3324
Test:  [  70/2050]  eta: 0:29:12    time: 0.8183  data: 0.0087  max mem: 3324
Test:  [  80/2050]  eta: 0:28:51    time: 0.8096  data: 0.0086  max mem: 3324
Test:  [  90/2050]  eta: 0:29:05    time: 0.9096  data: 0.0096  max mem: 3324
Test:  [ 100/2050]  eta: 0:28:42    time: 0.9016  data: 0.0096  max mem: 3324
Test:  [ 110/2050]  eta: 0:28:37    time: 0.8627  data: 0.0094  max mem: 3324
Test:  [ 120/2050]  eta: 0:28:29    time: 0.8960  data: 0.0099  max mem: 3324
Test:  [ 130/2050]  eta: 0:28:08    time: 0.8464  data: 0.0094  max mem: 3324
Test:  [ 140/2050]  eta: 0:27:46    time: 0.7927  data: 0.0088  max mem: 3324
Test:  [ 150/2050]  eta: 0:27:34    time: 0.8148  data: 0.0090  max mem: 3324
Test:  [ 160/2050]  eta: 0:27:27    time: 0.8672  data: 0.0095  max mem: 3324
Test:  [ 170/2050]  eta: 0:27:25    time: 0.9066  data: 0.0099  max mem: 3324
Test:  [ 180/2050]  eta: 0:27:08    time: 0.8627  data: 0.0095  max mem: 3324
Test:  [ 190/2050]  eta: 0:26:59    time: 0.8321  data: 0.0094  max mem: 3324
Test:  [ 200/2050]  eta: 0:26:52    time: 0.8783  data: 0.0099  max mem: 3324
Test:  [ 210/2050]  eta: 0:26:44    time: 0.8868  data: 0.0098  max mem: 3324
Test:  [ 220/2050]  eta: 0:26:32    time: 0.8572  data: 0.0098  max mem: 3324
Test:  [ 230/2050]  eta: 0:26:24    time: 0.8519  data: 0.0097  max mem: 3324
Test:  [ 240/2050]  eta: 0:26:17    time: 0.8852  data: 0.0097  max mem: 3324
Test:  [ 250/2050]  eta: 0:26:11    time: 0.9050  data: 0.0101  max mem: 3324
Test:  [ 260/2050]  eta: 0:26:03    time: 0.9002  data: 0.0101  max mem: 3324
Test:  [ 270/2050]  eta: 0:25:52    time: 0.8625  data: 0.0096  max mem: 3324
Test:  [ 280/2050]  eta: 0:25:42    time: 0.8412  data: 0.0095  max mem: 3324
Test:  [ 290/2050]  eta: 0:25:36    time: 0.8827  data: 0.0095  max mem: 3324
Test:  [ 300/2050]  eta: 0:25:32    time: 0.9371  data: 0.0102  max mem: 3324
Test:  [ 310/2050]  eta: 0:25:22    time: 0.9024  data: 0.0102  max mem: 3324
Test:  [ 320/2050]  eta: 0:25:14    time: 0.8715  data: 0.0097  max mem: 3324
Test:  [ 330/2050]  eta: 0:25:08    time: 0.9045  data: 0.0100  max mem: 3324
Test:  [ 340/2050]  eta: 0:25:00    time: 0.9095  data: 0.0100  max mem: 3324
Test:  [ 350/2050]  eta: 0:24:48    time: 0.8532  data: 0.0096  max mem: 3324
Test:  [ 360/2050]  eta: 0:24:40    time: 0.8480  data: 0.0094  max mem: 3324
Test:  [ 370/2050]  eta: 0:24:28    time: 0.8458  data: 0.0093  max mem: 3324
Test:  [ 380/2050]  eta: 0:24:16    time: 0.8019  data: 0.0090  max mem: 3324
Test:  [ 390/2050]  eta: 0:24:09    time: 0.8639  data: 0.0098  max mem: 3324
Test:  [ 400/2050]  eta: 0:24:01    time: 0.9049  data: 0.0102  max mem: 3324
Test:  [ 410/2050]  eta: 0:23:53    time: 0.8860  data: 0.0098  max mem: 3324
Test:  [ 420/2050]  eta: 0:23:45    time: 0.8988  data: 0.0099  max mem: 3324
Test:  [ 430/2050]  eta: 0:23:36    time: 0.8849  data: 0.0097  max mem: 3324
Test:  [ 440/2050]  eta: 0:23:28    time: 0.8788  data: 0.0099  max mem: 3324
Test:  [ 450/2050]  eta: 0:23:18    time: 0.8714  data: 0.0099  max mem: 3324
Test:  [ 460/2050]  eta: 0:23:09    time: 0.8492  data: 0.0096  max mem: 3324
Test:  [ 470/2050]  eta: 0:22:58    time: 0.8375  data: 0.0094  max mem: 3324
Test:  [ 480/2050]  eta: 0:22:52    time: 0.8827  data: 0.0098  max mem: 3324
Test:  [ 490/2050]  eta: 0:22:46    time: 0.9574  data: 0.0105  max mem: 3324
Test:  [ 500/2050]  eta: 0:22:38    time: 0.9331  data: 0.0102  max mem: 3324
Test:  [ 510/2050]  eta: 0:22:27    time: 0.8387  data: 0.0092  max mem: 3324
Test:  [ 520/2050]  eta: 0:22:16    time: 0.7921  data: 0.0089  max mem: 3324
Test:  [ 530/2050]  eta: 0:22:06    time: 0.8169  data: 0.0093  max mem: 3324
Test:  [ 540/2050]  eta: 0:21:59    time: 0.8890  data: 0.0099  max mem: 3324
Test:  [ 550/2050]  eta: 0:21:51    time: 0.9192  data: 0.0102  max mem: 3324
Test:  [ 560/2050]  eta: 0:21:44    time: 0.9135  data: 0.0103  max mem: 3627
Test:  [ 570/2050]  eta: 0:21:35    time: 0.9074  data: 0.0102  max mem: 3627
Test:  [ 580/2050]  eta: 0:21:25    time: 0.8539  data: 0.0094  max mem: 3627
Test:  [ 590/2050]  eta: 0:21:16    time: 0.8503  data: 0.0095  max mem: 3627
Test:  [ 600/2050]  eta: 0:21:07    time: 0.8610  data: 0.0096  max mem: 3627
Test:  [ 610/2050]  eta: 0:20:59    time: 0.8801  data: 0.0097  max mem: 3627
Test:  [ 620/2050]  eta: 0:20:51    time: 0.9093  data: 0.0102  max mem: 3627
Test:  [ 630/2050]  eta: 0:20:42    time: 0.8748  data: 0.0098  max mem: 3627
Test:  [ 640/2050]  eta: 0:20:33    time: 0.8601  data: 0.0096  max mem: 3627
Test:  [ 650/2050]  eta: 0:20:25    time: 0.9017  data: 0.0101  max mem: 3627
Test:  [ 660/2050]  eta: 0:20:18    time: 0.9353  data: 0.0103  max mem: 3627
Test:  [ 670/2050]  eta: 0:20:08    time: 0.8865  data: 0.0098  max mem: 3627
Test:  [ 680/2050]  eta: 0:20:00    time: 0.8602  data: 0.0098  max mem: 3627
Test:  [ 690/2050]  eta: 0:19:50    time: 0.8694  data: 0.0096  max mem: 3627
Test:  [ 700/2050]  eta: 0:19:42    time: 0.8736  data: 0.0095  max mem: 3627
Test:  [ 710/2050]  eta: 0:19:34    time: 0.9021  data: 0.0103  max mem: 3627
Test:  [ 720/2050]  eta: 0:19:25    time: 0.8942  data: 0.0102  max mem: 3627
Test:  [ 730/2050]  eta: 0:19:17    time: 0.8901  data: 0.0099  max mem: 3627
Test:  [ 740/2050]  eta: 0:19:08    time: 0.8815  data: 0.0098  max mem: 3627
Test:  [ 750/2050]  eta: 0:18:58    time: 0.8365  data: 0.0095  max mem: 3627
Test:  [ 760/2050]  eta: 0:18:51    time: 0.8989  data: 0.0099  max mem: 3627
Test:  [ 770/2050]  eta: 0:18:43    time: 0.9403  data: 0.0104  max mem: 3627
Test:  [ 780/2050]  eta: 0:18:33    time: 0.8678  data: 0.0097  max mem: 3627
Test:  [ 790/2050]  eta: 0:18:24    time: 0.8430  data: 0.0092  max mem: 3627
Test:  [ 800/2050]  eta: 0:18:16    time: 0.8898  data: 0.0099  max mem: 3627
Test:  [ 810/2050]  eta: 0:18:07    time: 0.9110  data: 0.0102  max mem: 3627
Test:  [ 820/2050]  eta: 0:17:58    time: 0.8521  data: 0.0095  max mem: 3627
Test:  [ 830/2050]  eta: 0:17:49    time: 0.8550  data: 0.0096  max mem: 3627
Test:  [ 840/2050]  eta: 0:17:40    time: 0.8474  data: 0.0093  max mem: 3627
Test:  [ 850/2050]  eta: 0:17:31    time: 0.8656  data: 0.0095  max mem: 3627
Test:  [ 860/2050]  eta: 0:17:24    time: 0.9412  data: 0.0108  max mem: 3627
Test:  [ 870/2050]  eta: 0:17:15    time: 0.9176  data: 0.0105  max mem: 3627
Test:  [ 880/2050]  eta: 0:17:05    time: 0.8235  data: 0.0090  max mem: 3627
Test:  [ 890/2050]  eta: 0:16:56    time: 0.8352  data: 0.0091  max mem: 3627
Test:  [ 900/2050]  eta: 0:16:48    time: 0.8870  data: 0.0099  max mem: 3627
Test:  [ 910/2050]  eta: 0:16:38    time: 0.8440  data: 0.0096  max mem: 3627
Test:  [ 920/2050]  eta: 0:16:29    time: 0.8202  data: 0.0092  max mem: 3627
Test:  [ 930/2050]  eta: 0:16:20    time: 0.8480  data: 0.0095  max mem: 3627
Test:  [ 940/2050]  eta: 0:16:12    time: 0.8934  data: 0.0100  max mem: 3627
Test:  [ 950/2050]  eta: 0:16:03    time: 0.8941  data: 0.0097  max mem: 3627
Test:  [ 960/2050]  eta: 0:15:54    time: 0.8555  data: 0.0096  max mem: 3627
Test:  [ 970/2050]  eta: 0:15:44    time: 0.8421  data: 0.0096  max mem: 3627
Test:  [ 980/2050]  eta: 0:15:35    time: 0.8288  data: 0.0094  max mem: 3627
Test:  [ 990/2050]  eta: 0:15:26    time: 0.8407  data: 0.0093  max mem: 3627
Test:  [1000/2050]  eta: 0:15:18    time: 0.9127  data: 0.0101  max mem: 3627
Test:  [1010/2050]  eta: 0:15:11    time: 0.9755  data: 0.0109  max mem: 3627
Test:  [1020/2050]  eta: 0:15:02    time: 0.9541  data: 0.0109  max mem: 3627
Test:  [1030/2050]  eta: 0:14:54    time: 0.9275  data: 0.0107  max mem: 3627
Test:  [1040/2050]  eta: 0:14:46    time: 0.9539  data: 0.0106  max mem: 3627
Test:  [1050/2050]  eta: 0:14:39    time: 0.9903  data: 0.0108  max mem: 3627
Test:  [1060/2050]  eta: 0:14:30    time: 0.9463  data: 0.0108  max mem: 3627
Test:  [1070/2050]  eta: 0:14:21    time: 0.8491  data: 0.0097  max mem: 3627
Test:  [1080/2050]  eta: 0:14:12    time: 0.8484  data: 0.0094  max mem: 3627
Test:  [1090/2050]  eta: 0:14:03    time: 0.8702  data: 0.0097  max mem: 3627
Test:  [1100/2050]  eta: 0:13:55    time: 0.9381  data: 0.0102  max mem: 3627
Test:  [1110/2050]  eta: 0:13:46    time: 0.9259  data: 0.0101  max mem: 3627
Test:  [1120/2050]  eta: 0:13:39    time: 0.9202  data: 0.0102  max mem: 3627
Test:  [1130/2050]  eta: 0:13:29    time: 0.9173  data: 0.0103  max mem: 3627
Test:  [1140/2050]  eta: 0:13:20    time: 0.8197  data: 0.0093  max mem: 3627
Test:  [1150/2050]  eta: 0:13:11    time: 0.8319  data: 0.0091  max mem: 3627
Test:  [1160/2050]  eta: 0:13:03    time: 0.8998  data: 0.0100  max mem: 3627
Test:  [1170/2050]  eta: 0:12:54    time: 0.9201  data: 0.0104  max mem: 3627
Test:  [1180/2050]  eta: 0:12:46    time: 0.9311  data: 0.0102  max mem: 3627
Test:  [1190/2050]  eta: 0:12:37    time: 0.9177  data: 0.0102  max mem: 3627
Test:  [1200/2050]  eta: 0:12:29    time: 0.9284  data: 0.0102  max mem: 3627
Test:  [1210/2050]  eta: 0:12:21    time: 0.9810  data: 0.0107  max mem: 3627
Test:  [1220/2050]  eta: 0:12:13    time: 0.9877  data: 0.0111  max mem: 3627
Test:  [1230/2050]  eta: 0:12:05    time: 1.0246  data: 0.0113  max mem: 3627
Test:  [1240/2050]  eta: 0:11:57    time: 1.0063  data: 0.0110  max mem: 3627
Test:  [1250/2050]  eta: 0:11:48    time: 0.9314  data: 0.0105  max mem: 3627
Test:  [1260/2050]  eta: 0:11:39    time: 0.9207  data: 0.0102  max mem: 3627
Test:  [1270/2050]  eta: 0:11:30    time: 0.8937  data: 0.0099  max mem: 3627
Test:  [1280/2050]  eta: 0:11:22    time: 0.8864  data: 0.0098  max mem: 3627
Test:  [1290/2050]  eta: 0:11:13    time: 0.8889  data: 0.0098  max mem: 3627
Test:  [1300/2050]  eta: 0:11:04    time: 0.8588  data: 0.0096  max mem: 3627
Test:  [1310/2050]  eta: 0:10:55    time: 0.8787  data: 0.0096  max mem: 3627
Test:  [1320/2050]  eta: 0:10:46    time: 0.8891  data: 0.0101  max mem: 3627
Test:  [1330/2050]  eta: 0:10:38    time: 0.9330  data: 0.0104  max mem: 3627
Test:  [1340/2050]  eta: 0:10:29    time: 0.9227  data: 0.0103  max mem: 3627
Test:  [1350/2050]  eta: 0:10:20    time: 0.9115  data: 0.0101  max mem: 3627
Test:  [1360/2050]  eta: 0:10:11    time: 0.8979  data: 0.0099  max mem: 3627
Test:  [1370/2050]  eta: 0:10:02    time: 0.8396  data: 0.0094  max mem: 3627
Test:  [1380/2050]  eta: 0:09:53    time: 0.8938  data: 0.0099  max mem: 3627
Test:  [1390/2050]  eta: 0:09:44    time: 0.9105  data: 0.0102  max mem: 3627
Test:  [1400/2050]  eta: 0:09:35    time: 0.8576  data: 0.0097  max mem: 3627
Test:  [1410/2050]  eta: 0:09:27    time: 0.8758  data: 0.0096  max mem: 3627
Test:  [1420/2050]  eta: 0:09:18    time: 0.9032  data: 0.0097  max mem: 3627
Test:  [1430/2050]  eta: 0:09:10    time: 0.9656  data: 0.0106  max mem: 3627
Test:  [1440/2050]  eta: 0:09:01    time: 0.9881  data: 0.0110  max mem: 3627
Test:  [1450/2050]  eta: 0:08:52    time: 0.9654  data: 0.0107  max mem: 3627
Test:  [1460/2050]  eta: 0:08:44    time: 0.9880  data: 0.0109  max mem: 3627
Test:  [1470/2050]  eta: 0:08:35    time: 0.9694  data: 0.0107  max mem: 3627
Test:  [1480/2050]  eta: 0:08:27    time: 0.9485  data: 0.0106  max mem: 3627
Test:  [1490/2050]  eta: 0:08:18    time: 0.9088  data: 0.0101  max mem: 3627
Test:  [1500/2050]  eta: 0:08:09    time: 0.9002  data: 0.0100  max mem: 3627
Test:  [1510/2050]  eta: 0:08:00    time: 0.9191  data: 0.0102  max mem: 3627
Test:  [1520/2050]  eta: 0:07:51    time: 0.9332  data: 0.0103  max mem: 3627
Test:  [1530/2050]  eta: 0:07:43    time: 1.0019  data: 0.0110  max mem: 3627
Test:  [1540/2050]  eta: 0:07:34    time: 1.0130  data: 0.0114  max mem: 3627
Test:  [1550/2050]  eta: 0:07:26    time: 0.9720  data: 0.0107  max mem: 3627
Test:  [1560/2050]  eta: 0:07:17    time: 0.9358  data: 0.0102  max mem: 3627
Test:  [1570/2050]  eta: 0:07:08    time: 0.8813  data: 0.0098  max mem: 3627
Test:  [1580/2050]  eta: 0:06:59    time: 0.9101  data: 0.0099  max mem: 3627
Test:  [1590/2050]  eta: 0:06:50    time: 0.9446  data: 0.0104  max mem: 3627
Test:  [1600/2050]  eta: 0:06:41    time: 0.9179  data: 0.0102  max mem: 3627
Test:  [1610/2050]  eta: 0:06:33    time: 0.9234  data: 0.0102  max mem: 3627
Test:  [1620/2050]  eta: 0:06:24    time: 0.9543  data: 0.0107  max mem: 3627
Test:  [1630/2050]  eta: 0:06:15    time: 0.9317  data: 0.0103  max mem: 3627
Test:  [1640/2050]  eta: 0:06:06    time: 0.8982  data: 0.0100  max mem: 3627
Test:  [1650/2050]  eta: 0:05:57    time: 0.8941  data: 0.0099  max mem: 3627
Test:  [1660/2050]  eta: 0:05:49    time: 0.9815  data: 0.0106  max mem: 3627
Test:  [1670/2050]  eta: 0:05:40    time: 1.0090  data: 0.0110  max mem: 3627
Test:  [1680/2050]  eta: 0:05:31    time: 0.9336  data: 0.0103  max mem: 3627
Test:  [1690/2050]  eta: 0:05:22    time: 0.9279  data: 0.0103  max mem: 3627
Test:  [1700/2050]  eta: 0:05:13    time: 0.8601  data: 0.0095  max mem: 3627
Test:  [1710/2050]  eta: 0:05:04    time: 0.8533  data: 0.0094  max mem: 3627
Test:  [1720/2050]  eta: 0:04:55    time: 0.9131  data: 0.0101  max mem: 3627
Test:  [1730/2050]  eta: 0:04:46    time: 0.9205  data: 0.0102  max mem: 3627
Test:  [1740/2050]  eta: 0:04:37    time: 0.9469  data: 0.0105  max mem: 3627
Test:  [1750/2050]  eta: 0:04:28    time: 0.9788  data: 0.0109  max mem: 3627
Test:  [1760/2050]  eta: 0:04:20    time: 0.9747  data: 0.0108  max mem: 3627
Test:  [1770/2050]  eta: 0:04:10    time: 0.8826  data: 0.0099  max mem: 3627
Test:  [1780/2050]  eta: 0:04:02    time: 0.8916  data: 0.0101  max mem: 3627
Test:  [1790/2050]  eta: 0:03:53    time: 0.9276  data: 0.0103  max mem: 3627
Test:  [1800/2050]  eta: 0:03:44    time: 0.8734  data: 0.0097  max mem: 3627
Test:  [1810/2050]  eta: 0:03:35    time: 0.8703  data: 0.0096  max mem: 3627
Test:  [1820/2050]  eta: 0:03:26    time: 0.8952  data: 0.0100  max mem: 3627
Test:  [1830/2050]  eta: 0:03:17    time: 0.9097  data: 0.0101  max mem: 3627
Test:  [1840/2050]  eta: 0:03:08    time: 0.8516  data: 0.0094  max mem: 3627
Test:  [1850/2050]  eta: 0:02:59    time: 0.8693  data: 0.0097  max mem: 3627
Test:  [1860/2050]  eta: 0:02:50    time: 0.9095  data: 0.0100  max mem: 3627
Test:  [1870/2050]  eta: 0:02:41    time: 0.8899  data: 0.0101  max mem: 3627
Test:  [1880/2050]  eta: 0:02:32    time: 0.8386  data: 0.0093  max mem: 3627
Test:  [1890/2050]  eta: 0:02:23    time: 0.8381  data: 0.0095  max mem: 3627
Test:  [1900/2050]  eta: 0:02:14    time: 0.9190  data: 0.0102  max mem: 3627
Test:  [1910/2050]  eta: 0:02:05    time: 0.8970  data: 0.0099  max mem: 3627
Test:  [1920/2050]  eta: 0:01:56    time: 0.9303  data: 0.0104  max mem: 3627
Test:  [1930/2050]  eta: 0:01:47    time: 0.9712  data: 0.0108  max mem: 3627
Test:  [1940/2050]  eta: 0:01:38    time: 0.8562  data: 0.0096  max mem: 3627
Test:  [1950/2050]  eta: 0:01:29    time: 0.8298  data: 0.0090  max mem: 3627
Test:  [1960/2050]  eta: 0:01:20    time: 0.9168  data: 0.0100  max mem: 3627
Test:  [1970/2050]  eta: 0:01:11    time: 0.9646  data: 0.0106  max mem: 3627
Test:  [1980/2050]  eta: 0:01:02    time: 0.9487  data: 0.0106  max mem: 3627
Test:  [1990/2050]  eta: 0:00:53    time: 0.9055  data: 0.0101  max mem: 3627
Test:  [2000/2050]  eta: 0:00:44    time: 0.9127  data: 0.0102  max mem: 3627
Test:  [2010/2050]  eta: 0:00:35    time: 0.9805  data: 0.0109  max mem: 3627
Test:  [2020/2050]  eta: 0:00:26    time: 0.9988  data: 0.0112  max mem: 3627
Test:  [2030/2050]  eta: 0:00:17    time: 0.9796  data: 0.0109  max mem: 3627
Test:  [2040/2050]  eta: 0:00:08    time: 0.9537  data: 0.0102  max mem: 3627
Test:  [2049/2050]  eta: 0:00:00    time: 0.8917  data: 0.0102  max mem: 3627
Test: Total time: 0:30:41 (0.8981 s / it)
aeroplane has 903 predictions.
bicycle has 1727 predictions.
bird has 1622 predictions.
boat has 1794 predictions.
bottle has 5784 predictions.
bus has 1041 predictions.
car has 9837 predictions.
cat has 1356 predictions.
chair has 11146 predictions.
cow has 1065 predictions.
diningtable has 4858 predictions.
dog has 1747 predictions.
horse has 809 predictions.
motorbike has 1161 predictions.
person has 39158 predictions.
pottedplant has 2924 predictions.
sheep has 774 predictions.
sofa has 3779 predictions.
train has 786 predictions.
tvmonitor has 1847 predictions.
truck has 3075 predictions.
traffic light has 1728 predictions.
fire hydrant has 844 predictions.
stop sign has 607 predictions.
parking meter has 760 predictions.
bench has 4396 predictions.
elephant has 1400 predictions.
bear has 824 predictions.
zebra has 979 predictions.
giraffe has 890 predictions.
backpack has 3355 predictions.
umbrella has 1498 predictions.
handbag has 4135 predictions.
tie has 1430 predictions.
suitcase has 1282 predictions.
microwave has 957 predictions.
oven has 1931 predictions.
toaster has 1095 predictions.
sink has 1638 predictions.
refrigerator has 1579 predictions.
frisbee has 935 predictions.
skis has 1459 predictions.
snowboard has 1099 predictions.
sports ball has 1494 predictions.
kite has 1711 predictions.
baseball bat has 777 predictions.
baseball glove has 783 predictions.
skateboard has 1062 predictions.
surfboard has 1840 predictions.
tennis racket has 1106 predictions.
banana has 2353 predictions.
apple has 2420 predictions.
sandwich has 2675 predictions.
orange has 1688 predictions.
broccoli has 2891 predictions.
carrot has 2494 predictions.
hot dog has 1912 predictions.
pizza has 2230 predictions.
donut has 1782 predictions.
cake has 3360 predictions.
bed has 0 predictions.
toilet has 0 predictions.
laptop has 0 predictions.
mouse has 0 predictions.
remote has 0 predictions.
keyboard has 0 predictions.
cell phone has 0 predictions.
book has 0 predictions.
clock has 0 predictions.
vase has 0 predictions.
scissors has 0 predictions.
teddy bear has 0 predictions.
hair drier has 0 predictions.
toothbrush has 0 predictions.
wine glass has 0 predictions.
cup has 0 predictions.
fork has 0 predictions.
knife has 0 predictions.
spoon has 0 predictions.
bowl has 0 predictions.
unknown has 860008 predictions.
detection mAP50: 26.685579
detection mAP: 26.685579
---AP50---
Wilderness Impact: {0.1: {50: 0.004298826536540026}, 0.2: {50: 0.009842947094159369}, 0.3: {50: 0.01689827898727269}, 0.4: {50: 0.01754731618416678}, 0.5: {50: 0.017072448072580893}, 0.6: {50: 0.016154327310966266}, 0.7: {50: 0.01475459624389934}, 0.8: {50: 0.015123131828312798}, 0.9: {50: 0.015390226112326768}}
avg_precision: {0.1: {50: 0.008257345041597485}, 0.2: {50: 0.0022231082287098194}, 0.3: {50: 0.0022231082287098194}, 0.4: {50: 0.0022231082287098194}, 0.5: {50: 0.0022231082287098194}, 0.6: {50: 0.0022231082287098194}, 0.7: {50: 0.0022231082287098194}, 0.8: {50: 0.0022231082287098194}, 0.9: {50: 0.0022231082287098194}}
Absolute OSE (total_num_unk_det_as_known): {50: 2667.0}
total_num_unk 9299
AP50: ['65.7', '52.5', '59.7', '41.1', '29.1', '65.8', '53.1', '74.5', '26.0', '68.2', '21.9', '69.5', '77.2', '62.2', '50.5', '31.3', '64.5', '48.3', '77.0', '50.3', '20.8', '24.9', '55.6', '52.5', '35.0', '12.2', '64.9', '55.5', '77.4', '76.3', '4.1', '24.5', '5.3', '16.8', '21.4', '19.6', '13.7', '10.6', '15.9', '20.3', '46.3', '19.2', '11.2', '35.5', '19.3', '23.5', '32.7', '29.8', '29.7', '43.8', '15.7', '8.0', '15.6', '14.1', '15.6', '11.7', '13.1', '21.7', '25.3', '13.8', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.2']
Precisions50: ['27.9', '23.9', '31.1', '16.6', '16.0', '29.1', '22.6', '32.5', '12.8', '25.6', '11.0', '32.7', '44.3', '33.7', '29.9', '16.2', '33.0', '11.2', '39.7', '23.4', '7.5', '17.1', '8.9', '8.1', '4.5', '3.2', '16.1', '7.8', '23.9', '22.4', '2.9', '13.4', '3.1', '7.6', '9.8', '4.1', '4.4', '0.5', '7.5', '5.3', '9.5', '7.7', '3.1', '9.8', '10.9', '8.4', '9.7', '10.5', '7.9', '13.6', '7.5', '3.4', '4.4', '7.8', '6.5', '6.5', '3.1', '9.7', '12.1', '5.7', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.2']
Recall50: ['74.6', '63.0', '70.3', '59.7', '42.4', '76.0', '68.3', '85.9', '40.2', '83.1', '37.9', '85.8', '83.6', '70.5', '65.0', '49.8', '75.5', '74.3', '84.7', '67.5', '48.7', '40.6', '69.4', '63.6', '54.0', '22.3', '86.9', '87.7', '86.7', '85.0', '17.3', '38.8', '16.5', '29.4', '34.9', '36.4', '28.8', '29.4', '28.7', '39.3', '64.5', '35.9', '41.0', '45.8', '39.2', '33.3', '42.7', '47.6', '47.2', '56.4', '30.2', '20.8', '35.5', '27.1', '34.8', '22.7', '28.1', '41.9', '41.3', '29.3', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '19.4']
Prev class AP50: tensor(42.8964)
Prev class Precisions50: 17.27591812199995
Prev class Recall50: 57.564904472912495
Current class AP50: tensor(22.2731)
Current class Precisions50: 7.890676981446376
Current class Recall50: 38.26957205398013
Known AP50: tensor(36.0220)
Known Precisions50: 14.147504408482096
Known Recall50: 51.133126999935044
Unknown AP50: tensor(0.2121)
Unknown Precisions50: 0.20941665658924102
Unknown Recall50: 19.367673943434777
aeroplane 65.707573
bicycle 52.493362
bird 59.731030
boat 41.121670
bottle 29.127916
bus 65.823479
car 53.088177
cat 74.524315
chair 26.036716
cow 68.225105
diningtable 21.949327
dog 69.509148
horse 77.187195
motorbike 62.222588
person 50.469677
pottedplant 31.319645
sheep 64.521751
sofa 48.297493
train 77.023071
tvmonitor 50.330788
truck 20.773417
traffic light 24.874531
fire hydrant 55.622437
stop sign 52.476597
parking meter 35.036293
bench 12.159949
elephant 64.926804
bear 55.520313
zebra 77.445114
giraffe 76.252487
backpack 4.051182
umbrella 24.540695
handbag 5.311811
tie 16.782875
suitcase 21.350401
microwave 19.553690
oven 13.698752
toaster 10.606061
sink 15.862049
refrigerator 20.301785
frisbee 46.271168
skis 19.195932
snowboard 11.187165
sports ball 35.474236
kite 19.261963
baseball bat 23.533783
baseball glove 32.730125
skateboard 29.786354
surfboard 29.668734
tennis racket 43.813004
banana 15.701386
apple 7.954081
sandwich 15.559612
orange 14.061131
broccoli 15.629846
carrot 11.692781
hot dog 13.108356
pizza 21.733879
donut 25.283411
cake 13.815507
bed 0.000000
toilet 0.000000
laptop 0.000000
mouse 0.000000
remote 0.000000
keyboard 0.000000
cell phone 0.000000
book 0.000000
clock 0.000000
vase 0.000000
scissors 0.000000
teddy bear 0.000000
hair drier 0.000000
toothbrush 0.000000
wine glass 0.000000
cup 0.000000
fork 0.000000
knife 0.000000
spoon 0.000000
bowl 0.000000
unknown 0.212116
+ PY_ARGS=
+ python -u main_open_world.py --output_dir exps/MOWODB/PROB/eval --dataset TOWOD --PREV_INTRODUCED_CLS 60 --CUR_INTRODUCED_CLS 20 --train_set owod_t1_train --test_set owod_all_task_test --epochs 191 --lr_drop 35 --model_type prob --obj_loss_coef 8e-4 --obj_temp 1.3 --pretrain exps/MOWODB/PROB/t4.pth --eval --wandb_project ''
{'OWDETR': ('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown'), 'TOWOD': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown'), 'VOC2007': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown')}
('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown')
| distributed init (rank 0): env://
git:
  sha: 5675c9e44c999734c9465041c4c1f191dda931f9, status: has uncommited changes, branch: main

Namespace(lr=0.0002, lr_backbone_names=['backbone.0'], lr_backbone=2e-05, lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, batch_size=5, weight_decay=0.0001, epochs=191, lr_drop=35, lr_drop_epochs=None, clip_max_norm=0.1, sgd=False, with_box_refine=False, two_stage=False, masks=False, backbone='dino_resnet50', frozen_weights=None, dilation=False, position_embedding='sine', position_embedding_scale=6.283185307179586, num_feature_levels=4, enc_layers=6, dec_layers=6, dim_feedforward=1024, hidden_dim=256, dropout=0.1, nheads=8, num_queries=100, dec_n_points=4, enc_n_points=4, aux_loss=True, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, cls_loss_coef=2, bbox_loss_coef=5, giou_loss_coef=2, focal_alpha=0.25, coco_panoptic_path=None, remove_difficult=False, output_dir='exps/MOWODB/PROB/eval', device='cuda', seed=42, resume='', start_epoch=0, eval=True, viz=False, eval_every=5, num_workers=3, cache_mode=False, PREV_INTRODUCED_CLS=60, CUR_INTRODUCED_CLS=20, unmatched_boxes=False, top_unk=5, featdim=1024, invalid_cls_logits=False, NC_branch=False, bbox_thresh=0.3, pretrain='exps/MOWODB/PROB/t4.pth', nc_loss_coef=2, train_set='owod_t1_train', test_set='owod_all_task_test', num_classes=81, nc_epoch=0, dataset='TOWOD', data_root='./data/OWOD', unk_conf_w=1.0, model_type='prob', wandb_name='', wandb_project='', obj_loss_coef=0.0008, obj_temp=1.3, freeze_prob_model=False, num_inst_per_class=50, exemplar_replay_selection=False, exemplar_replay_max_length=10000000000.0, exemplar_replay_dir='', exemplar_replay_prev_file='', exemplar_replay_cur_file='', exemplar_replay_random=False, rank=0, world_size=1, gpu=0, dist_url='env://', distributed=True, dist_backend='nccl')
Invalid class range: []
DINO resnet50
/home/hannalukas/miniconda3/envs/prob/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/hannalukas/miniconda3/envs/prob/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
running with exemplar_replay_selection
DeformableDETR(
  (transformer): DeformableTransformer(
    (encoder): DeformableTransformerEncoder(
      (layers): ModuleList(
        (0): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (decoder): DeformableTransformerDecoder(
      (layers): ModuleList(
        (0): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (reference_points): Linear(in_features=256, out_features=2, bias=True)
  )
  (class_embed): ModuleList(
    (0): Linear(in_features=256, out_features=81, bias=True)
    (1): Linear(in_features=256, out_features=81, bias=True)
    (2): Linear(in_features=256, out_features=81, bias=True)
    (3): Linear(in_features=256, out_features=81, bias=True)
    (4): Linear(in_features=256, out_features=81, bias=True)
    (5): Linear(in_features=256, out_features=81, bias=True)
  )
  (bbox_embed): ModuleList(
    (0): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (1): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (2): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (3): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (4): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (5): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (prob_obj_head): ModuleList(
    (0): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (1): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (2): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (3): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (4): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (5): ProbObjectnessHead(
      (flatten): Flatten(start_dim=0, end_dim=1)
      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (query_embed): Embedding(100, 512)
  (input_proj): ModuleList(
    (0): Sequential(
      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (1): Sequential(
      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (2): Sequential(
      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (3): Sequential(
      (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
  )
  (backbone): Joiner(
    (0): Backbone(
      (body): IntermediateLayerGetter(
        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
        (bn1): FrozenBatchNorm2d()
        (relu): ReLU(inplace=True)
        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
      )
    )
    (1): PositionEmbeddingSine()
  )
)
number of params: 39742295
TOWOD
owod_t1_train
owod_all_task_test
Dataset OWDetection
    Number of datapoints: 16551
    Root location: ./data/OWOD
    [['train'], Compose(
    <datasets.transforms.RandomHorizontalFlip object at 0x7926d0259990>
    <datasets.transforms.RandomSelect object at 0x7926d01eafb0>
    Compose(
    <datasets.transforms.ToTensor object at 0x7926d026ef50>
    <datasets.transforms.Normalize object at 0x7927a1780b50>
)
)]
Dataset OWDetection
    Number of datapoints: 10246
    Root location: ./data/OWOD
    [['test'], Compose(
    <datasets.transforms.RandomResize object at 0x7926d01eaa10>
    Compose(
    <datasets.transforms.ToTensor object at 0x7926d01eabf0>
    <datasets.transforms.Normalize object at 0x7926d01eabc0>
)
)]
Initialized from the pre-training model
<All keys matched successfully>
testing data details
81
80
('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl')
('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown')
/home/hannalukas/Documents/PROB/PROB_exp/models/position_encoding.py:49: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)
/home/hannalukas/miniconda3/envs/prob/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/hannalukas/Documents/PROB/PROB_exp/models/prob_deformable_detr.py:537: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  topk_boxes = topk_indexes // out_logits.shape[2]
Test:  [   0/2050]  eta: 1:16:30    time: 2.2390  data: 0.4518  max mem: 2694
Test:  [  10/2050]  eta: 0:35:55    time: 1.0568  data: 0.0501  max mem: 3324
Test:  [  20/2050]  eta: 0:33:23    time: 0.9241  data: 0.0101  max mem: 3324
Test:  [  30/2050]  eta: 0:31:47    time: 0.8825  data: 0.0100  max mem: 3324
Test:  [  40/2050]  eta: 0:31:13    time: 0.8744  data: 0.0098  max mem: 3324
Test:  [  50/2050]  eta: 0:30:22    time: 0.8594  data: 0.0098  max mem: 3324
Test:  [  60/2050]  eta: 0:29:53    time: 0.8390  data: 0.0096  max mem: 3324
Test:  [  70/2050]  eta: 0:29:12    time: 0.8191  data: 0.0092  max mem: 3324
Test:  [  80/2050]  eta: 0:28:51    time: 0.8107  data: 0.0089  max mem: 3324
Test:  [  90/2050]  eta: 0:29:05    time: 0.9104  data: 0.0096  max mem: 3324
Test:  [ 100/2050]  eta: 0:28:43    time: 0.9021  data: 0.0094  max mem: 3324
Test:  [ 110/2050]  eta: 0:28:38    time: 0.8627  data: 0.0091  max mem: 3324
Test:  [ 120/2050]  eta: 0:28:29    time: 0.8952  data: 0.0094  max mem: 3324
Test:  [ 130/2050]  eta: 0:28:08    time: 0.8456  data: 0.0090  max mem: 3324
Test:  [ 140/2050]  eta: 0:27:46    time: 0.7929  data: 0.0087  max mem: 3324
Test:  [ 150/2050]  eta: 0:27:34    time: 0.8149  data: 0.0089  max mem: 3324
Test:  [ 160/2050]  eta: 0:27:27    time: 0.8672  data: 0.0093  max mem: 3324
Test:  [ 170/2050]  eta: 0:27:25    time: 0.9069  data: 0.0098  max mem: 3324
Test:  [ 180/2050]  eta: 0:27:08    time: 0.8629  data: 0.0093  max mem: 3324
Test:  [ 190/2050]  eta: 0:26:59    time: 0.8324  data: 0.0092  max mem: 3324
Test:  [ 200/2050]  eta: 0:26:52    time: 0.8783  data: 0.0096  max mem: 3324
Test:  [ 210/2050]  eta: 0:26:44    time: 0.8867  data: 0.0096  max mem: 3324
Test:  [ 220/2050]  eta: 0:26:32    time: 0.8574  data: 0.0097  max mem: 3324
Test:  [ 230/2050]  eta: 0:26:24    time: 0.8516  data: 0.0095  max mem: 3324
Test:  [ 240/2050]  eta: 0:26:17    time: 0.8850  data: 0.0095  max mem: 3324
Test:  [ 250/2050]  eta: 0:26:11    time: 0.9049  data: 0.0098  max mem: 3324
Test:  [ 260/2050]  eta: 0:26:03    time: 0.9001  data: 0.0100  max mem: 3324
Test:  [ 270/2050]  eta: 0:25:52    time: 0.8625  data: 0.0094  max mem: 3324
Test:  [ 280/2050]  eta: 0:25:42    time: 0.8413  data: 0.0095  max mem: 3324
Test:  [ 290/2050]  eta: 0:25:36    time: 0.8833  data: 0.0096  max mem: 3324
Test:  [ 300/2050]  eta: 0:25:32    time: 0.9375  data: 0.0101  max mem: 3324
Test:  [ 310/2050]  eta: 0:25:22    time: 0.9025  data: 0.0101  max mem: 3324
Test:  [ 320/2050]  eta: 0:25:14    time: 0.8717  data: 0.0096  max mem: 3324
Test:  [ 330/2050]  eta: 0:25:08    time: 0.9049  data: 0.0099  max mem: 3324
Test:  [ 340/2050]  eta: 0:25:00    time: 0.9108  data: 0.0103  max mem: 3324
Test:  [ 350/2050]  eta: 0:24:48    time: 0.8549  data: 0.0102  max mem: 3324
Test:  [ 360/2050]  eta: 0:24:40    time: 0.8497  data: 0.0100  max mem: 3324
Test:  [ 370/2050]  eta: 0:24:28    time: 0.8478  data: 0.0101  max mem: 3324
Test:  [ 380/2050]  eta: 0:24:16    time: 0.8039  data: 0.0097  max mem: 3324
Test:  [ 390/2050]  eta: 0:24:10    time: 0.8661  data: 0.0106  max mem: 3324
Test:  [ 400/2050]  eta: 0:24:02    time: 0.9069  data: 0.0111  max mem: 3324
Test:  [ 410/2050]  eta: 0:23:53    time: 0.8873  data: 0.0104  max mem: 3324
Test:  [ 420/2050]  eta: 0:23:46    time: 0.8993  data: 0.0101  max mem: 3324
Test:  [ 430/2050]  eta: 0:23:37    time: 0.8858  data: 0.0098  max mem: 3324
Test:  [ 440/2050]  eta: 0:23:29    time: 0.8805  data: 0.0104  max mem: 3324
Test:  [ 450/2050]  eta: 0:23:19    time: 0.8725  data: 0.0103  max mem: 3324
Test:  [ 460/2050]  eta: 0:23:10    time: 0.8502  data: 0.0098  max mem: 3324
Test:  [ 470/2050]  eta: 0:22:59    time: 0.8385  data: 0.0095  max mem: 3324
Test:  [ 480/2050]  eta: 0:22:53    time: 0.8837  data: 0.0102  max mem: 3324
Test:  [ 490/2050]  eta: 0:22:47    time: 0.9584  data: 0.0111  max mem: 3324
Test:  [ 500/2050]  eta: 0:22:39    time: 0.9338  data: 0.0106  max mem: 3324
Test:  [ 510/2050]  eta: 0:22:27    time: 0.8396  data: 0.0095  max mem: 3324
Test:  [ 520/2050]  eta: 0:22:16    time: 0.7927  data: 0.0090  max mem: 3324
Test:  [ 530/2050]  eta: 0:22:07    time: 0.8173  data: 0.0094  max mem: 3324
Test:  [ 540/2050]  eta: 0:22:00    time: 0.8905  data: 0.0105  max mem: 3324
Test:  [ 550/2050]  eta: 0:21:52    time: 0.9205  data: 0.0107  max mem: 3324
Test:  [ 560/2050]  eta: 0:21:44    time: 0.9134  data: 0.0104  max mem: 3627
Test:  [ 570/2050]  eta: 0:21:36    time: 0.9081  data: 0.0103  max mem: 3627
Test:  [ 580/2050]  eta: 0:21:26    time: 0.8554  data: 0.0096  max mem: 3627
Test:  [ 590/2050]  eta: 0:21:17    time: 0.8517  data: 0.0098  max mem: 3627
Test:  [ 600/2050]  eta: 0:21:08    time: 0.8623  data: 0.0099  max mem: 3627
Test:  [ 610/2050]  eta: 0:21:00    time: 0.8814  data: 0.0102  max mem: 3627
Test:  [ 620/2050]  eta: 0:20:52    time: 0.9104  data: 0.0108  max mem: 3627
Test:  [ 630/2050]  eta: 0:20:42    time: 0.8752  data: 0.0100  max mem: 3627
Test:  [ 640/2050]  eta: 0:20:34    time: 0.8610  data: 0.0101  max mem: 3627
Test:  [ 650/2050]  eta: 0:20:26    time: 0.9027  data: 0.0106  max mem: 3627
Test:  [ 660/2050]  eta: 0:20:19    time: 0.9359  data: 0.0104  max mem: 3627
Test:  [ 670/2050]  eta: 0:20:09    time: 0.8872  data: 0.0099  max mem: 3627
Test:  [ 680/2050]  eta: 0:20:01    time: 0.8605  data: 0.0099  max mem: 3627
Test:  [ 690/2050]  eta: 0:19:51    time: 0.8700  data: 0.0098  max mem: 3627
Test:  [ 700/2050]  eta: 0:19:43    time: 0.8747  data: 0.0100  max mem: 3627
Test:  [ 710/2050]  eta: 0:19:35    time: 0.9029  data: 0.0107  max mem: 3627
Test:  [ 720/2050]  eta: 0:19:26    time: 0.8946  data: 0.0103  max mem: 3627
Test:  [ 730/2050]  eta: 0:19:18    time: 0.8905  data: 0.0099  max mem: 3627
Test:  [ 740/2050]  eta: 0:19:09    time: 0.8821  data: 0.0099  max mem: 3627
Test:  [ 750/2050]  eta: 0:18:59    time: 0.8375  data: 0.0099  max mem: 3627
Test:  [ 760/2050]  eta: 0:18:52    time: 0.8996  data: 0.0104  max mem: 3627
Test:  [ 770/2050]  eta: 0:18:43    time: 0.9405  data: 0.0106  max mem: 3627
Test:  [ 780/2050]  eta: 0:18:34    time: 0.8685  data: 0.0099  max mem: 3627
Test:  [ 790/2050]  eta: 0:18:25    time: 0.8437  data: 0.0094  max mem: 3627
Test:  [ 800/2050]  eta: 0:18:17    time: 0.8902  data: 0.0101  max mem: 3627
Test:  [ 810/2050]  eta: 0:18:08    time: 0.9117  data: 0.0103  max mem: 3627
Test:  [ 820/2050]  eta: 0:17:59    time: 0.8526  data: 0.0097  max mem: 3627
Test:  [ 830/2050]  eta: 0:17:50    time: 0.8550  data: 0.0098  max mem: 3627
Test:  [ 840/2050]  eta: 0:17:40    time: 0.8474  data: 0.0094  max mem: 3627
Test:  [ 850/2050]  eta: 0:17:32    time: 0.8667  data: 0.0099  max mem: 3627
Test:  [ 860/2050]  eta: 0:17:25    time: 0.9423  data: 0.0112  max mem: 3627
Test:  [ 870/2050]  eta: 0:17:16    time: 0.9176  data: 0.0106  max mem: 3627
Test:  [ 880/2050]  eta: 0:17:05    time: 0.8236  data: 0.0091  max mem: 3627
Test:  [ 890/2050]  eta: 0:16:57    time: 0.8356  data: 0.0093  max mem: 3627
Test:  [ 900/2050]  eta: 0:16:48    time: 0.8872  data: 0.0101  max mem: 3627
Test:  [ 910/2050]  eta: 0:16:39    time: 0.8439  data: 0.0095  max mem: 3627
Test:  [ 920/2050]  eta: 0:16:29    time: 0.8201  data: 0.0090  max mem: 3627
Test:  [ 930/2050]  eta: 0:16:20    time: 0.8476  data: 0.0093  max mem: 3627
Test:  [ 940/2050]  eta: 0:16:12    time: 0.8932  data: 0.0096  max mem: 3627
Test:  [ 950/2050]  eta: 0:16:03    time: 0.8940  data: 0.0094  max mem: 3627
Test:  [ 960/2050]  eta: 0:15:54    time: 0.8553  data: 0.0094  max mem: 3627
Test:  [ 970/2050]  eta: 0:15:45    time: 0.8419  data: 0.0094  max mem: 3627
Test:  [ 980/2050]  eta: 0:15:36    time: 0.8289  data: 0.0092  max mem: 3627
Test:  [ 990/2050]  eta: 0:15:27    time: 0.8407  data: 0.0090  max mem: 3627
Test:  [1000/2050]  eta: 0:15:19    time: 0.9124  data: 0.0098  max mem: 3627
Test:  [1010/2050]  eta: 0:15:11    time: 0.9752  data: 0.0105  max mem: 3627
Test:  [1020/2050]  eta: 0:15:03    time: 0.9536  data: 0.0105  max mem: 3627
Test:  [1030/2050]  eta: 0:14:55    time: 0.9267  data: 0.0101  max mem: 3627
Test:  [1040/2050]  eta: 0:14:47    time: 0.9531  data: 0.0101  max mem: 3627
Test:  [1050/2050]  eta: 0:14:39    time: 0.9898  data: 0.0105  max mem: 3627
Test:  [1060/2050]  eta: 0:14:31    time: 0.9462  data: 0.0104  max mem: 3627
Test:  [1070/2050]  eta: 0:14:21    time: 0.8493  data: 0.0094  max mem: 3627
Test:  [1080/2050]  eta: 0:14:12    time: 0.8482  data: 0.0091  max mem: 3627
Test:  [1090/2050]  eta: 0:14:03    time: 0.8698  data: 0.0094  max mem: 3627
Test:  [1100/2050]  eta: 0:13:56    time: 0.9376  data: 0.0099  max mem: 3627
Test:  [1110/2050]  eta: 0:13:47    time: 0.9256  data: 0.0098  max mem: 3627
Test:  [1120/2050]  eta: 0:13:39    time: 0.9202  data: 0.0100  max mem: 3627
Test:  [1130/2050]  eta: 0:13:30    time: 0.9172  data: 0.0101  max mem: 3627
Test:  [1140/2050]  eta: 0:13:20    time: 0.8194  data: 0.0089  max mem: 3627
Test:  [1150/2050]  eta: 0:13:11    time: 0.8318  data: 0.0088  max mem: 3627
Test:  [1160/2050]  eta: 0:13:03    time: 0.8996  data: 0.0098  max mem: 3627
Test:  [1170/2050]  eta: 0:12:54    time: 0.9197  data: 0.0101  max mem: 3627
Test:  [1180/2050]  eta: 0:12:46    time: 0.9314  data: 0.0100  max mem: 3627
Test:  [1190/2050]  eta: 0:12:37    time: 0.9182  data: 0.0101  max mem: 3627
Test:  [1200/2050]  eta: 0:12:29    time: 0.9284  data: 0.0101  max mem: 3627
Test:  [1210/2050]  eta: 0:12:21    time: 0.9807  data: 0.0103  max mem: 3627
Test:  [1220/2050]  eta: 0:12:13    time: 0.9876  data: 0.0108  max mem: 3627
Test:  [1230/2050]  eta: 0:12:05    time: 1.0244  data: 0.0111  max mem: 3627
Test:  [1240/2050]  eta: 0:11:57    time: 1.0060  data: 0.0106  max mem: 3627
Test:  [1250/2050]  eta: 0:11:48    time: 0.9315  data: 0.0102  max mem: 3627
Test:  [1260/2050]  eta: 0:11:40    time: 0.9211  data: 0.0100  max mem: 3627
Test:  [1270/2050]  eta: 0:11:31    time: 0.8942  data: 0.0098  max mem: 3627
Test:  [1280/2050]  eta: 0:11:22    time: 0.8876  data: 0.0103  max mem: 3627
Test:  [1290/2050]  eta: 0:11:13    time: 0.8903  data: 0.0106  max mem: 3627
Test:  [1300/2050]  eta: 0:11:04    time: 0.8594  data: 0.0100  max mem: 3627
Test:  [1310/2050]  eta: 0:10:55    time: 0.8805  data: 0.0105  max mem: 3627
Test:  [1320/2050]  eta: 0:10:46    time: 0.8913  data: 0.0111  max mem: 3627
Test:  [1330/2050]  eta: 0:10:38    time: 0.9330  data: 0.0103  max mem: 3627
Test:  [1340/2050]  eta: 0:10:29    time: 0.9223  data: 0.0098  max mem: 3627
Test:  [1350/2050]  eta: 0:10:20    time: 0.9116  data: 0.0099  max mem: 3627
Test:  [1360/2050]  eta: 0:10:11    time: 0.8975  data: 0.0096  max mem: 3627
Test:  [1370/2050]  eta: 0:10:02    time: 0.8392  data: 0.0091  max mem: 3627
Test:  [1380/2050]  eta: 0:09:54    time: 0.8938  data: 0.0097  max mem: 3627
Test:  [1390/2050]  eta: 0:09:45    time: 0.9108  data: 0.0100  max mem: 3627
Test:  [1400/2050]  eta: 0:09:36    time: 0.8577  data: 0.0095  max mem: 3627
Test:  [1410/2050]  eta: 0:09:27    time: 0.8755  data: 0.0094  max mem: 3627
Test:  [1420/2050]  eta: 0:09:18    time: 0.9031  data: 0.0096  max mem: 3627
Test:  [1430/2050]  eta: 0:09:10    time: 0.9655  data: 0.0105  max mem: 3627
Test:  [1440/2050]  eta: 0:09:01    time: 0.9882  data: 0.0108  max mem: 3627
Test:  [1450/2050]  eta: 0:08:53    time: 0.9658  data: 0.0105  max mem: 3627
Test:  [1460/2050]  eta: 0:08:44    time: 0.9880  data: 0.0107  max mem: 3627
Test:  [1470/2050]  eta: 0:08:36    time: 0.9693  data: 0.0105  max mem: 3627
Test:  [1480/2050]  eta: 0:08:27    time: 0.9486  data: 0.0102  max mem: 3627
Test:  [1490/2050]  eta: 0:08:18    time: 0.9085  data: 0.0098  max mem: 3627
Test:  [1500/2050]  eta: 0:08:09    time: 0.8998  data: 0.0097  max mem: 3627
Test:  [1510/2050]  eta: 0:08:00    time: 0.9190  data: 0.0101  max mem: 3627
Test:  [1520/2050]  eta: 0:07:52    time: 0.9326  data: 0.0101  max mem: 3627
Test:  [1530/2050]  eta: 0:07:43    time: 1.0016  data: 0.0107  max mem: 3627
Test:  [1540/2050]  eta: 0:07:35    time: 1.0135  data: 0.0112  max mem: 3627
Test:  [1550/2050]  eta: 0:07:26    time: 0.9723  data: 0.0105  max mem: 3627
Test:  [1560/2050]  eta: 0:07:17    time: 0.9360  data: 0.0102  max mem: 3627
Test:  [1570/2050]  eta: 0:07:08    time: 0.8813  data: 0.0097  max mem: 3627
Test:  [1580/2050]  eta: 0:06:59    time: 0.9098  data: 0.0096  max mem: 3627
Test:  [1590/2050]  eta: 0:06:50    time: 0.9446  data: 0.0102  max mem: 3627
Test:  [1600/2050]  eta: 0:06:42    time: 0.9180  data: 0.0100  max mem: 3627
Test:  [1610/2050]  eta: 0:06:33    time: 0.9238  data: 0.0101  max mem: 3627
Test:  [1620/2050]  eta: 0:06:24    time: 0.9548  data: 0.0106  max mem: 3627
Test:  [1630/2050]  eta: 0:06:15    time: 0.9317  data: 0.0101  max mem: 3627
Test:  [1640/2050]  eta: 0:06:06    time: 0.8980  data: 0.0097  max mem: 3627
Test:  [1650/2050]  eta: 0:05:57    time: 0.8943  data: 0.0097  max mem: 3627
Test:  [1660/2050]  eta: 0:05:49    time: 0.9823  data: 0.0105  max mem: 3627
Test:  [1670/2050]  eta: 0:05:40    time: 1.0094  data: 0.0109  max mem: 3627
Test:  [1680/2050]  eta: 0:05:31    time: 0.9336  data: 0.0102  max mem: 3627
Test:  [1690/2050]  eta: 0:05:22    time: 0.9278  data: 0.0102  max mem: 3627
Test:  [1700/2050]  eta: 0:05:13    time: 0.8602  data: 0.0095  max mem: 3627
Test:  [1710/2050]  eta: 0:05:04    time: 0.8534  data: 0.0094  max mem: 3627
Test:  [1720/2050]  eta: 0:04:55    time: 0.9130  data: 0.0099  max mem: 3627
Test:  [1730/2050]  eta: 0:04:46    time: 0.9206  data: 0.0099  max mem: 3627
Test:  [1740/2050]  eta: 0:04:37    time: 0.9465  data: 0.0102  max mem: 3627
Test:  [1750/2050]  eta: 0:04:29    time: 0.9782  data: 0.0106  max mem: 3627
Test:  [1760/2050]  eta: 0:04:20    time: 0.9746  data: 0.0105  max mem: 3627
Test:  [1770/2050]  eta: 0:04:11    time: 0.8824  data: 0.0098  max mem: 3627
Test:  [1780/2050]  eta: 0:04:02    time: 0.8909  data: 0.0097  max mem: 3627
Test:  [1790/2050]  eta: 0:03:53    time: 0.9270  data: 0.0100  max mem: 3627
Test:  [1800/2050]  eta: 0:03:44    time: 0.8732  data: 0.0096  max mem: 3627
Test:  [1810/2050]  eta: 0:03:35    time: 0.8694  data: 0.0092  max mem: 3627
Test:  [1820/2050]  eta: 0:03:26    time: 0.8943  data: 0.0096  max mem: 3627
Test:  [1830/2050]  eta: 0:03:17    time: 0.9098  data: 0.0099  max mem: 3627
Test:  [1840/2050]  eta: 0:03:08    time: 0.8513  data: 0.0093  max mem: 3627
Test:  [1850/2050]  eta: 0:02:59    time: 0.8690  data: 0.0094  max mem: 3627
Test:  [1860/2050]  eta: 0:02:50    time: 0.9095  data: 0.0098  max mem: 3627
Test:  [1870/2050]  eta: 0:02:41    time: 0.8896  data: 0.0098  max mem: 3627
Test:  [1880/2050]  eta: 0:02:32    time: 0.8382  data: 0.0091  max mem: 3627
Test:  [1890/2050]  eta: 0:02:23    time: 0.8372  data: 0.0091  max mem: 3627
Test:  [1900/2050]  eta: 0:02:14    time: 0.9184  data: 0.0099  max mem: 3627
Test:  [1910/2050]  eta: 0:02:05    time: 0.8970  data: 0.0097  max mem: 3627
Test:  [1920/2050]  eta: 0:01:56    time: 0.9302  data: 0.0101  max mem: 3627
Test:  [1930/2050]  eta: 0:01:47    time: 0.9713  data: 0.0106  max mem: 3627
Test:  [1940/2050]  eta: 0:01:38    time: 0.8564  data: 0.0094  max mem: 3627
Test:  [1950/2050]  eta: 0:01:29    time: 0.8299  data: 0.0089  max mem: 3627
Test:  [1960/2050]  eta: 0:01:20    time: 0.9164  data: 0.0097  max mem: 3627
Test:  [1970/2050]  eta: 0:01:11    time: 0.9644  data: 0.0103  max mem: 3627
Test:  [1980/2050]  eta: 0:01:02    time: 0.9489  data: 0.0104  max mem: 3627
Test:  [1990/2050]  eta: 0:00:53    time: 0.9052  data: 0.0098  max mem: 3627
Test:  [2000/2050]  eta: 0:00:44    time: 0.9121  data: 0.0099  max mem: 3627
Test:  [2010/2050]  eta: 0:00:35    time: 0.9797  data: 0.0105  max mem: 3627
Test:  [2020/2050]  eta: 0:00:26    time: 0.9982  data: 0.0109  max mem: 3627
Test:  [2030/2050]  eta: 0:00:17    time: 0.9797  data: 0.0107  max mem: 3627
Test:  [2040/2050]  eta: 0:00:08    time: 0.9540  data: 0.0102  max mem: 3627
Test:  [2049/2050]  eta: 0:00:00    time: 0.8923  data: 0.0101  max mem: 3627
Test: Total time: 0:30:41 (0.8983 s / it)
aeroplane has 1892 predictions.
bicycle has 3239 predictions.
bird has 3816 predictions.
boat has 2836 predictions.
bottle has 7362 predictions.
bus has 1460 predictions.
car has 14191 predictions.
cat has 1920 predictions.
chair has 15461 predictions.
cow has 2689 predictions.
diningtable has 5641 predictions.
dog has 2717 predictions.
horse has 2080 predictions.
motorbike has 1893 predictions.
person has 46975 predictions.
pottedplant has 5187 predictions.
sheep has 1553 predictions.
sofa has 5234 predictions.
train has 1340 predictions.
tvmonitor has 2657 predictions.
truck has 5949 predictions.
traffic light has 2779 predictions.
fire hydrant has 1433 predictions.
stop sign has 961 predictions.
parking meter has 2144 predictions.
bench has 4816 predictions.
elephant has 1867 predictions.
bear has 1227 predictions.
zebra has 1700 predictions.
giraffe has 1684 predictions.
backpack has 4299 predictions.
umbrella has 2554 predictions.
handbag has 6176 predictions.
tie has 1799 predictions.
suitcase has 1980 predictions.
microwave has 751 predictions.
oven has 2013 predictions.
toaster has 1057 predictions.
sink has 2490 predictions.
refrigerator has 1474 predictions.
frisbee has 966 predictions.
skis has 2448 predictions.
snowboard has 1849 predictions.
sports ball has 1727 predictions.
kite has 2766 predictions.
baseball bat has 1754 predictions.
baseball glove has 1663 predictions.
skateboard has 1336 predictions.
surfboard has 2547 predictions.
tennis racket has 1914 predictions.
banana has 3459 predictions.
apple has 2193 predictions.
sandwich has 2066 predictions.
orange has 1658 predictions.
broccoli has 2949 predictions.
carrot has 2729 predictions.
hot dog has 2295 predictions.
pizza has 2099 predictions.
donut has 1346 predictions.
cake has 2135 predictions.
bed has 6734 predictions.
toilet has 3076 predictions.
laptop has 2451 predictions.
mouse has 2228 predictions.
remote has 3257 predictions.
keyboard has 2271 predictions.
cell phone has 6361 predictions.
book has 11802 predictions.
clock has 4467 predictions.
vase has 5052 predictions.
scissors has 2508 predictions.
teddy bear has 2806 predictions.
hair drier has 2699 predictions.
toothbrush has 3236 predictions.
wine glass has 3106 predictions.
cup has 8071 predictions.
fork has 3087 predictions.
knife has 5648 predictions.
spoon has 3877 predictions.
bowl has 6955 predictions.
unknown has 713713 predictions.
/home/hannalukas/Documents/PROB/PROB_exp/datasets/open_world_eval.py:430: RuntimeWarning: invalid value encountered in divide
  rec = tp / float(npos)
detection mAP50: 31.192247
detection mAP: 31.192247
---AP50---
Wilderness Impact: {0.1: {50: 0.0}, 0.2: {50: 0.0}, 0.3: {50: 0.0}, 0.4: {50: 0.0}, 0.5: {50: 0.0}, 0.6: {50: 0.0}, 0.7: {50: 0.0}, 0.8: {50: 0.0}, 0.9: {50: 0.0}}
avg_precision: {0.1: {50: 0.0}, 0.2: {50: 0.0}, 0.3: {50: 0.0}, 0.4: {50: 0.0}, 0.5: {50: 0.0}, 0.6: {50: 0.0}, 0.7: {50: 0.0}, 0.8: {50: 0.0}, 0.9: {50: 0.0}}
Absolute OSE (total_num_unk_det_as_known): {50: 0.0}
total_num_unk 0
AP50: ['62.0', '53.0', '59.5', '40.2', '29.2', '64.8', '52.6', '73.5', '25.4', '66.4', '21.8', '69.0', '77.2', '59.6', '49.7', '34.0', '64.4', '49.9', '73.1', '58.6', '21.6', '24.6', '51.9', '49.6', '43.5', '11.8', '66.4', '52.5', '76.1', '73.3', '4.8', '27.7', '10.1', '18.1', '24.0', '19.3', '10.4', '11.6', '16.4', '17.8', '41.2', '21.2', '9.9', '32.9', '22.2', '23.7', '31.2', '28.8', '30.0', '43.1', '16.9', '10.9', '15.1', '15.3', '15.4', '11.6', '11.2', '17.5', '20.5', '11.0', '23.8', '31.2', '33.9', '38.6', '17.7', '30.8', '12.4', '11.2', '32.8', '15.6', '10.8', '28.3', '9.3', '5.7', '14.7', '15.6', '13.8', '10.4', '9.8', '15.1', '0.0']
Precisions50: ['12.9', '13.1', '13.2', '10.1', '12.6', '20.2', '15.9', '23.7', '9.3', '10.1', '9.0', '21.3', '17.4', '20.6', '25.1', '9.8', '16.6', '8.1', '22.7', '17.3', '4.2', '10.2', '5.2', '4.6', '1.9', '2.9', '11.7', '5.2', '13.9', '11.8', '2.3', '8.2', '2.2', '5.4', '7.3', '5.5', '3.8', '0.6', '5.5', '5.4', '8.7', '4.8', '1.9', '7.7', '6.9', '4.3', '5.0', '8.2', '5.8', '8.2', '5.3', '3.2', '5.1', '7.8', '5.7', '5.7', '3.0', '9.7', '12.4', '6.9', '1.7', '4.6', '6.7', '3.5', '3.9', '4.7', '2.3', '3.3', '3.6', '2.6', '0.6', '4.7', '0.1', '0.7', '5.3', '6.6', '3.7', '2.2', '2.1', '5.5', '0.0']
Recall50: ['72.5', '65.1', '72.3', '58.5', '42.5', '74.7', '69.9', '88.8', '41.0', '86.3', '36.4', '87.3', '87.3', '70.9', '65.5', '53.9', '78.3', '74.9', '82.7', '71.6', '53.2', '39.0', '68.5', '57.1', '65.1', '22.2', '84.6', '87.7', '87.4', '84.6', '17.9', '40.5', '17.5', '26.7', '40.5', '38.3', '26.1', '35.3', '32.2', '36.9', '60.9', '37.5', '42.2', '41.4', '40.3', '38.5', '46.6', '47.2', '47.6', '58.6', '31.1', '18.0', '31.6', '26.7', '31.2', '21.8', '32.4', '39.2', '32.1', '22.7', '60.3', '54.7', '56.4', '65.3', '38.2', '60.8', '38.2', '25.7', '44.7', '34.5', '30.4', '53.2', '23.5', '27.3', '29.7', '33.5', '28.0', '18.5', '17.6', '31.2', 'nan']
Prev class AP50: tensor(35.7509)
Prev class Precisions50: 9.221036980759377
Prev class Recall50: 51.48863348062154
Current class AP50: tensor(19.0759)
Current class Precisions50: 3.4178068051551804
Current class Recall50: 38.58494985413422
Known AP50: tensor(31.5821)
Known Precisions50: 7.77022943685833
Known Recall50: 48.26271257399971
Unknown AP50: tensor(0.)
Unknown Precisions50: 0.0
Unknown Recall50: nan
aeroplane 61.989887
bicycle 52.981079
bird 59.476028
boat 40.233471
bottle 29.212008
bus 64.849159
car 52.564056
cat 73.526451
chair 25.367594
cow 66.396103
diningtable 21.843073
dog 68.955345
horse 77.153816
motorbike 59.591900
person 49.731941
pottedplant 33.957279
sheep 64.355606
sofa 49.851978
train 73.137367
tvmonitor 58.606880
truck 21.587099
traffic light 24.630766
fire hydrant 51.904720
stop sign 49.629490
parking meter 43.497959
bench 11.841867
elephant 66.392487
bear 52.483513
zebra 76.053581
giraffe 73.308525
backpack 4.804864
umbrella 27.713293
handbag 10.066699
tie 18.121019
suitcase 24.029879
microwave 19.286833
oven 10.417128
toaster 11.646496
sink 16.427450
refrigerator 17.780102
frisbee 41.152283
skis 21.207958
snowboard 9.863271
sports ball 32.945877
kite 22.229012
baseball bat 23.706787
baseball glove 31.249416
skateboard 28.770128
surfboard 30.026936
tennis racket 43.116421
banana 16.923454
apple 10.865801
sandwich 15.119323
orange 15.267645
broccoli 15.413998
carrot 11.614225
hot dog 11.206939
pizza 17.471643
donut 20.525795
cake 10.971582
bed 23.808722
toilet 31.240076
laptop 33.901863
mouse 38.567657
remote 17.702602
keyboard 30.765387
cell phone 12.441363
book 11.211479
clock 32.764904
vase 15.616721
scissors 10.846128
teddy bear 28.277445
hair drier 9.308347
toothbrush 5.659700
wine glass 14.747911
cup 15.594509
fork 13.834117
knife 10.397727
spoon 9.762721
bowl 15.069464
unknown 0.000000
